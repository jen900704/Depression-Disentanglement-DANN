"""
DANN (Static Backbone) â€” Scenario A
=====================================
ä¿®æ­£ç‰ˆ v3ï¼šä¿®æ­£ Speaker Map é‚è¼¯ + evaluate åŠ  mask

Scenario A çš„ Speaker é‚è¼¯èªªæ˜ï¼š
  - Train setï¼šå…¨æ˜¯è·¯äººï¼ˆ~151ä½ï¼‰ï¼ŒGRL å°è·¯äººåš adversarial
  - Test setï¼š38ä½ Target Groupï¼ˆå…¨æ˜¯é™Œç”Ÿäººï¼Œtrain å¾æœªè¦‹éï¼‰
  - Test çš„ speaker_label = -1ï¼ˆä¸åƒèˆ‡ L_spkï¼Œå°æ‡‰è«–æ–‡ Spk Acc = N/Aï¼‰

ä¿®æ­£é …ç›®ï¼š
  [v3-1] speaker_to_idx.get(spk_id, 0) â†’ get(spk_id, -1)
         test çš„ 38 ä½é™Œç”Ÿäººä¸åœ¨ map â†’ -1ï¼Œè€ŒééŒ¯èª¤çš„ 0
  [v3-2] evaluate() åŠ  maskï¼Œåªå° s >= 0 è¨ˆç®— Spk Acc
         â†’ test å…¨æ˜¯ s=-1ï¼ŒSpk Acc å›å‚³ N/A (0.0)ï¼Œå°æ‡‰è«–æ–‡ Table 1
  [v3-3] training loop åŠ  maskï¼Œåªå° train è·¯äººï¼ˆs >= 0ï¼‰è¨ˆç®— L_spk
         â†’ test æ™‚ s=-1 ä¸æœƒæ±¡æŸ“ lossï¼ˆeval ä¸è¨ˆç®— lossï¼Œæ­¤é …ç¢ºä¿é‚è¼¯ä¸€è‡´ï¼‰

ä¸è®Šé …ç›®ï¼ˆèˆ‡ DANN-FT çš„å”¯ä¸€æ¶æ§‹å·®ç•°ï¼‰ï¼š
  - Wav2Vec2 å®Œå…¨å‡çµï¼ˆCNN + Transformer éƒ½ä¸æ›´æ–°ï¼‰
  - ç‰¹å¾µä¸€æ¬¡é æå–ï¼Œå­˜æˆ Tensor
  - Optimizer: Adam, lr=1e-3, batch=32, epochs=30
  - Alpha: global step æ¯”ä¾‹
  - Best checkpoint: val F1
"""

import os
import copy
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Function
from transformers import Wav2Vec2Processor, Wav2Vec2Model, set_seed
import torchaudio
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# ============================================================
#  è¨­å®šå€
# ============================================================
TRAIN_CSV_PATH = "./experiment_sisman_scientific/scenario_A_screening/train.csv"
TEST_CSV_PATH  = "./experiment_sisman_scientific/scenario_A_screening/test.csv"
AUDIO_ROOT     = ""

MODEL_NAME  = "facebook/wav2vec2-base"
DEVICE      = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE  = 32
EPOCHS      = 30
TOTAL_RUNS  = 5
SEED        = 103

print(f"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {DEVICE}")

# ============================================================
#  GRL
# ============================================================
class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None

class GradientReversalLayer(nn.Module):
    def forward(self, x, alpha=1.0):
        return GradientReversalFn.apply(x, alpha)

# ============================================================
#  æ¨¡å‹å®šç¾©
# ============================================================
class DANN_Model(nn.Module):
    def __init__(self, input_dim=768, hidden_dim=128, num_classes=2, num_speakers=151):
        super().__init__()
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
        )
        self.class_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes),
        )
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_speakers),
        )
        self.grl = GradientReversalLayer()

    def forward(self, x, alpha=1.0):
        features   = self.shared_encoder(x)
        class_out  = self.class_classifier(features)
        rev        = self.grl(features, alpha)
        domain_out = self.domain_classifier(rev)
        return class_out, domain_out

# ============================================================
#  Helper
# ============================================================
def extract_speaker_id(filepath):
    return os.path.basename(str(filepath)).split('_')[0]

def build_speaker_map(train_csv_path):
    """
    Scenario Aï¼šSpeaker Map å¾ train.csv å»ºç«‹ï¼ˆ~151 ä½è·¯äººï¼‰
    test çš„ 38 ä½é™Œç”Ÿäººä¸åœ¨æ­¤ mapï¼Œload æ™‚çµ¦ -1
    """
    df = pd.read_csv(train_csv_path)
    speakers = sorted(df['path'].apply(extract_speaker_id).unique())
    speaker_map = {spk: idx for idx, spk in enumerate(speakers)}
    print(f"ğŸ” [v3] Speaker Map å¾ train å»ºç«‹: {len(speaker_map)} ä½è·¯äºº")
    return speaker_map

# ============================================================
#  è³‡æ–™è™•ç†
# ============================================================
def prepare_data(csv_path, processor, w2v_model, speaker_to_idx):
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ æ­£åœ¨è™•ç† {csv_path} (å…± {len(df)} ç­†)...")

    features_list, labels_list, speaker_indices_list = [], [], []
    label_map = {'dep': 1, '1': 1, 1: 1, 'non': 0, '0': 0, 0: 0}

    w2v_model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Extracting Features"):
            wav_path  = os.path.join(AUDIO_ROOT, row['path'])
            raw_label = str(row['label']).strip().lower()
            if raw_label not in label_map:
                continue
            try:
                waveform, sr = torchaudio.load(wav_path)
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)
                if sr != 16000:
                    waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)

                inputs = processor(
                    waveform.squeeze().numpy(),
                    sampling_rate=16000, return_tensors="pt", padding=True
                )
                inputs     = {k: v.to(DEVICE) for k, v in inputs.items()}
                embeddings = w2v_model(**inputs).last_hidden_state.mean(dim=1).cpu()

                features_list.append(embeddings)
                labels_list.append(label_map[raw_label])

                # [v3-1] è·¯äººåœ¨ map â†’ æ­£ç¢º indexï¼›test é™Œç”Ÿäººä¸åœ¨ map â†’ -1
                spk_id = extract_speaker_id(wav_path)
                speaker_indices_list.append(speaker_to_idx.get(spk_id, -1))

            except Exception as e:
                print(f"âš ï¸ Error: {wav_path} -> {e}")

    if not features_list:
        raise ValueError("âŒ æ²’æœ‰ä»»ä½•è³‡æ–™è¢«æˆåŠŸè®€å–ï¼")

    X = torch.cat(features_list, dim=0)
    y = torch.tensor(labels_list,          dtype=torch.long)
    s = torch.tensor(speaker_indices_list, dtype=torch.long)

    n_known   = (s >= 0).sum().item()
    n_unknown = (s < 0).sum().item()
    print(f"   â†’ å·²çŸ¥è·¯äºº: {n_known} ç­† | é™Œç”Ÿäºº (s=-1): {n_unknown} ç­†")

    return X, y, s

# ============================================================
#  è©•ä¼°å‡½å¼
#  [v3-2] Spk Acc åªå° s >= 0 è¨ˆç®—
#  Scenario A çš„ test å…¨æ˜¯é™Œç”Ÿäºº(s=-1) â†’ Spk Acc = N/A (0.0)
# ============================================================
def evaluate(model, loader):
    model.eval()
    all_preds, all_labels = [], []
    correct_spk, total_spk = 0, 0
    with torch.no_grad():
        for inputs, labels, speakers in loader:
            inputs, labels, speakers = (
                inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
            )
            class_out, domain_out = model(inputs, alpha=0.0)
            _, preds     = torch.max(class_out,  1)
            _, spk_preds = torch.max(domain_out, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            # [v3-2] åªå°å·²çŸ¥ speaker è¨ˆç®— Spk Acc
            mask = speakers >= 0
            if mask.sum() > 0:
                correct_spk += (spk_preds[mask] == speakers[mask]).sum().item()
                total_spk   += mask.sum().item()

    acc     = accuracy_score(all_labels, all_preds)
    f1      = f1_score(all_labels, all_preds, average='macro')
    spk_acc = correct_spk / total_spk if total_spk > 0 else 0.0  # test å…¨ s=-1 â†’ 0.0 = N/A
    return acc, f1, spk_acc

# t-SNE ç”¨ç‰¹å¾µæŠ½å–
def get_feats(model, loader):
    model.eval()
    feats, labels_list = [], []
    with torch.no_grad():
        for inputs, labels, speakers in loader:
            inputs = inputs.to(DEVICE)
            f = model.shared_encoder(inputs).cpu().numpy()
            feats.append(f)
            labels_list.extend(labels.cpu().numpy())
    return np.vstack(feats), np.array(labels_list)

# ============================================================
#  ä¸»ç¨‹å¼
# ============================================================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ DANN (Static Backbone) â€” Scenario A  [v3 ä¿®æ­£ç‰ˆ]")
    print("   Speaker Map: train è·¯äººï¼ˆ~151ä½ï¼‰")
    print("   Test é™Œç”Ÿäºº s=-1ï¼Œä¸åƒèˆ‡ L_spkï¼ˆè«–æ–‡ Spk Acc = N/Aï¼‰")
    print("   Alpha: global step æ¯”ä¾‹ | Best checkpoint: val F1")
    print(f"   LR=1e-3 | Batch={BATCH_SIZE} | Epochs={EPOCHS} | Runs={TOTAL_RUNS}")
    print("=" * 60)

    # [v3] Speaker Map å¾ train å»ºç«‹ï¼ˆ~151ä½è·¯äººï¼‰
    speaker_map  = build_speaker_map(TRAIN_CSV_PATH)
    num_speakers = len(speaker_map)
    print(f"âœ… num_speakers = {num_speakers}")

    # ç‰¹å¾µæå–ï¼ˆåªåšä¸€æ¬¡ï¼‰
    print("\nğŸ§  è¼‰å…¥ Wav2Vec2ï¼ˆå‡çµï¼‰...")
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    w2v_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)

    print("\nğŸ“¦ é æå–ç‰¹å¾µï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    X_train, y_train, s_train = prepare_data(TRAIN_CSV_PATH, processor, w2v_model, speaker_map)
    X_test,  y_test,  s_test  = prepare_data(TEST_CSV_PATH,  processor, w2v_model, speaker_map)

    # é æœŸï¼štrain å…¨æ˜¯å·²çŸ¥è·¯äºº(s>=0)ï¼›test å…¨æ˜¯é™Œç”Ÿäºº(s=-1)
    print(f"âœ… Train: {len(X_train)} ç­† | Test: {len(X_test)} ç­†")

    del w2v_model
    torch.cuda.empty_cache()

    train_dataset = TensorDataset(X_train, y_train, s_train)
    test_dataset  = TensorDataset(X_test,  y_test,  s_test)

    all_run_accs = []
    all_run_f1s  = []

    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Run {run_i} / {TOTAL_RUNS}")
        print(f"{'='*60}")

        run_seed = SEED + run_i - 1
        set_seed(run_seed)

        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)

        total_steps = len(train_loader) * EPOCHS
        global_step = 0

        print("ğŸ—ï¸ åˆå§‹åŒ–å…¨æ–° DANN æ¨¡å‹...")
        dann_model = DANN_Model(num_speakers=num_speakers).to(DEVICE)

        optimizer        = optim.Adam(dann_model.parameters(), lr=1e-3)
        criterion_class  = nn.CrossEntropyLoss()
        criterion_domain = nn.CrossEntropyLoss()

        best_val_f1      = -1.0
        best_model_state = None

        print("âš”ï¸ é–‹å§‹è¨“ç·´...")
        for epoch in range(EPOCHS):
            dann_model.train()
            total_loss     = 0
            total_loss_dep = 0
            total_loss_spk = 0

            for inputs, labels, speakers in train_loader:
                inputs, labels, speakers = (
                    inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
                )

                p     = global_step / max(total_steps, 1)
                alpha = 2.0 / (1.0 + np.exp(-10 * p)) - 1
                global_step += 1

                optimizer.zero_grad()
                class_out, domain_out = dann_model(inputs, alpha=alpha)

                loss_dep = criterion_class(class_out, labels)

                # [v3-3] åªå° train è·¯äººï¼ˆs >= 0ï¼‰è¨ˆç®— L_spk
                # train set å…¨æ˜¯è·¯äººï¼Œmask æ‡‰å…¨ç‚º Trueï¼Œæ­¤ç‚ºé˜²ç¦¦æ€§å¯«æ³•
                mask = speakers >= 0
                if mask.sum() > 0:
                    loss_spk = criterion_domain(domain_out[mask], speakers[mask])
                else:
                    loss_spk = torch.tensor(0.0, device=DEVICE)

                loss = loss_dep + loss_spk
                loss.backward()
                optimizer.step()

                total_loss     += loss.item()
                total_loss_dep += loss_dep.item()
                total_loss_spk += loss_spk.item()

            acc, f1, spk_acc = evaluate(dann_model, test_loader)

            if f1 > best_val_f1:
                best_val_f1      = f1
                best_model_state = copy.deepcopy(dann_model.state_dict())

            if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:
                print(f"Epoch {epoch+1}/{EPOCHS} | "
                      f"Loss: {total_loss:.2f} (dep={total_loss_dep:.2f}, spk={total_loss_spk:.2f}) | "
                      f"Dep Acc: {acc:.4f} | F1: {f1:.4f} | "
                      f"Spk Acc: {spk_acc:.4f} (N/A for A) | Best F1: {best_val_f1:.4f}")

        print(f"\nğŸ† è¼‰å…¥ Best Checkpoint (F1={best_val_f1:.4f}) é€²è¡Œæœ€çµ‚è©•ä¼°...")
        dann_model.load_state_dict(best_model_state)
        final_acc, final_f1, final_spk_acc = evaluate(dann_model, test_loader)
        print(f"âœ… Run {run_i} æœ€çµ‚çµæœ â†’ Acc: {final_acc:.4f} | "
              f"F1: {final_f1:.4f} | Spk Acc: N/A")

        all_run_accs.append(final_acc)
        all_run_f1s.append(final_f1)

        # t-SNE â€” ç”¨ depression label ä¸Šè‰²ï¼ˆScenario A ç„¡ speaker identity å¯è¦–åŒ–æ„ç¾©ï¼‰
        print(f"\nğŸ¨ [Run {run_i}] ç¹ªè£½ t-SNE åœ–ï¼ˆdepression labelï¼‰...")
        dann_feats, dep_labels = get_feats(dann_model, test_loader)
        tsne    = TSNE(n_components=2, random_state=42 + run_i, perplexity=30)
        feats2d = tsne.fit_transform(dann_feats)

        plt.figure(figsize=(10, 8))
        sns.scatterplot(
            x=feats2d[:, 0], y=feats2d[:, 1],
            hue=dep_labels, palette={0: "steelblue", 1: "tomato"},
            legend="full"
        )
        plt.title(f"DANN (Static) Feature Space â€” Scenario A, Run {run_i}", fontsize=16)
        filename = f"tsne_dann_A_run_{run_i}.png"
        plt.savefig(filename)
        plt.close()
        print(f"âœ… åœ–ç‰‡å·²å„²å­˜: {filename}")

    # å½™ç¸½
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {TOTAL_RUNS} æ¬¡å¯¦é©—çµæœçµ±è¨ˆ (DANN Static Backbone â€” Scenario A v3)")
    print(f"{'='*60}")
    accs = np.array(all_run_accs)
    f1s  = np.array(all_run_f1s)
    for i, (a, f) in enumerate(zip(accs, f1s), 1):
        print(f"  Run {i}: Acc={a:.4f}, F1={f:.4f}")
    print(f"{'â”€'*40}")
    print(f"  å¹³å‡ Acc : {accs.mean():.4f} Â± {accs.std():.4f}")
    print(f"  å¹³å‡ F1  : {f1s.mean():.4f} Â± {f1s.std():.4f}")
    print(f"  Spk Acc  : N/Aï¼ˆScenario A æ¸¬è©¦å°è±¡ç‚ºå…¨æ–°é™Œç”Ÿäººï¼‰")
