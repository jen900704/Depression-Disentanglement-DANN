"""
DANN (Static Backbone) â€” Scenario A
=====================================
ä¿®æ­£ç‰ˆ v3ï¼Œå°é½Š Huang / DANN-FT çš„è©•ä¼°æ©Ÿåˆ¶ï¼š

ä¿®æ­£é …ç›®ï¼š
  1. Alpha schedule æ”¹ç‚º global step æ¯”ä¾‹ï¼ˆå°é½Š DANN-FTï¼‰
  2. åŠ å…¥ best checkpoint è¿½è¹¤ï¼ˆä»¥ val F1 ç‚ºæº–ï¼Œå°é½Šè«–æ–‡æŒ‡æ¨™ï¼‰
  3. çµæœå– best checkpoint è€Œéæœ€çµ‚ epoch

ä¸è®Šé …ç›®ï¼ˆåˆ»æ„ä¿ç•™ï¼Œèˆ‡ DANN-FT çš„å”¯ä¸€æ¶æ§‹å·®ç•°ï¼‰ï¼š
  - Wav2Vec2 å®Œå…¨å‡çµï¼ˆCNN + Transformer éƒ½ä¸æ›´æ–°ï¼‰
  - ç‰¹å¾µä¸€æ¬¡é æå–ï¼Œå­˜æˆ Tensorï¼ˆfrozen backbone ç„¡éœ€ backwardï¼‰
  - Optimizer: Adam, lr=1e-3, batch=32, epochs=30

èˆ‡ run_dann_B_v3.py çš„å”¯ä¸€å·®ç•°ï¼š
  - è³‡æ–™è·¯å¾‘ç‚º scenario_A_screening
  - t-SNE å­˜æª”åï¼štsne_dann_A_run_{i}.png
"""

import os
import copy
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Function
from transformers import Wav2Vec2Processor, Wav2Vec2Model, set_seed
import torchaudio
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# ============================================================
#  è¨­å®šå€
# ============================================================
TRAIN_CSV_PATH = "./experiment_sisman_scientific/scenario_A_screening/train.csv"
TEST_CSV_PATH  = "./experiment_sisman_scientific/scenario_A_screening/test.csv"
AUDIO_ROOT     = ""

MODEL_NAME  = "facebook/wav2vec2-base"
DEVICE      = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE  = 32
EPOCHS      = 30
TOTAL_RUNS  = 5
SEED        = 103

print(f"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {DEVICE}")

# ============================================================
#  GRL
# ============================================================
class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None

class GradientReversalLayer(nn.Module):
    def forward(self, x, alpha=1.0):
        return GradientReversalFn.apply(x, alpha)

# ============================================================
#  æ¨¡å‹å®šç¾©
# ============================================================
class DANN_Model(nn.Module):
    def __init__(self, input_dim=768, hidden_dim=128, num_classes=2, num_speakers=38):
        super().__init__()
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
        )
        self.class_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes),
        )
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_speakers),
        )
        self.grl = GradientReversalLayer()

    def forward(self, x, alpha=1.0):
        features   = self.shared_encoder(x)
        class_out  = self.class_classifier(features)
        rev        = self.grl(features, alpha)
        domain_out = self.domain_classifier(rev)
        return class_out, domain_out

# ============================================================
#  è³‡æ–™è™•ç†
# ============================================================
def extract_speaker_id(filepath):
    return os.path.basename(filepath).split('_')[0]

def prepare_data(csv_path, processor, model, speaker_to_idx=None, is_train=True):
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ æ­£åœ¨è™•ç† {csv_path} (å…± {len(df)} ç­†)...")

    features_list, labels_list, speaker_indices_list = [], [], []
    label_map = {'dep': 1, '1': 1, 1: 1, 'non': 0, '0': 0, 0: 0}

    if is_train and speaker_to_idx is None:
        all_speakers  = sorted(df['path'].apply(extract_speaker_id).unique())
        speaker_to_idx = {spk: idx for idx, spk in enumerate(all_speakers)}
        print(f"ğŸ” Speaker Map ({len(speaker_to_idx)} ä½èªªè©±è€…)")

    model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Extracting Features"):
            wav_path  = os.path.join(AUDIO_ROOT, row['path'])
            raw_label = str(row['label']).strip().lower()
            if raw_label not in label_map:
                continue
            try:
                waveform, sr = torchaudio.load(wav_path)
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)
                if sr != 16000:
                    waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)

                inputs = processor(
                    waveform.squeeze().numpy(),
                    sampling_rate=16000, return_tensors="pt", padding=True
                )
                inputs     = {k: v.to(DEVICE) for k, v in inputs.items()}
                embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu()

                features_list.append(embeddings)
                labels_list.append(label_map[raw_label])
                speaker_indices_list.append(
                    speaker_to_idx.get(extract_speaker_id(wav_path), 0)
                )
            except Exception as e:
                print(f"âš ï¸ Error: {wav_path} -> {e}")

    if not features_list:
        raise ValueError("âŒ æ²’æœ‰ä»»ä½•è³‡æ–™è¢«æˆåŠŸè®€å–ï¼")

    X = torch.cat(features_list, dim=0)
    y = torch.tensor(labels_list,         dtype=torch.long)
    s = torch.tensor(speaker_indices_list, dtype=torch.long)
    return X, y, s, speaker_to_idx

# ============================================================
#  è©•ä¼°å‡½å¼
# ============================================================
def evaluate(model, loader):
    model.eval()
    all_preds, all_labels = [], []
    correct_spk, total   = 0, 0
    with torch.no_grad():
        for inputs, labels, speakers in loader:
            inputs, labels, speakers = (
                inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
            )
            class_out, domain_out = model(inputs, alpha=0.0)
            _, preds     = torch.max(class_out,  1)
            _, spk_preds = torch.max(domain_out, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            correct_spk += (spk_preds == speakers).sum().item()
            total       += labels.size(0)

    acc     = accuracy_score(all_labels, all_preds)
    f1      = f1_score(all_labels, all_preds, average='macro')
    spk_acc = correct_spk / total
    return acc, f1, spk_acc

# t-SNE ç”¨ç‰¹å¾µæŠ½å–
def get_feats(model, loader):
    model.eval()
    feats, spks = [], []
    with torch.no_grad():
        for inputs, labels, speakers in loader:
            inputs = inputs.to(DEVICE)
            f = model.shared_encoder(inputs).cpu().numpy()
            feats.append(f)
            spks.extend(speakers.cpu().numpy())
    return np.vstack(feats), np.array(spks)

# ============================================================
#  ä¸»ç¨‹å¼
# ============================================================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ DANN (Static Backbone) â€” Scenario A  [v3 ä¿®æ­£ç‰ˆ]")
    print("   Alpha: global step æ¯”ä¾‹ | Best checkpoint: val F1")
    print(f"   LR=1e-3 | Batch={BATCH_SIZE} | Epochs={EPOCHS} | Runs={TOTAL_RUNS}")
    print("=" * 60)

    # ç‰¹å¾µæå–ï¼ˆåªåšä¸€æ¬¡ï¼‰
    print("\nğŸ§  è¼‰å…¥ Wav2Vec2ï¼ˆå‡çµï¼‰...")
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    w2v_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)

    print("\nğŸ“¦ é æå–ç‰¹å¾µï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    X_train, y_train, s_train, speaker_map = prepare_data(
        TRAIN_CSV_PATH, processor, w2v_model, is_train=True
    )
    X_test, y_test, s_test, _ = prepare_data(
        TEST_CSV_PATH, processor, w2v_model,
        speaker_to_idx=speaker_map, is_train=False
    )
    num_speakers = len(speaker_map)
    print(f"âœ… Train: {len(X_train)} ç­† | Test: {len(X_test)} ç­† | Speakers: {num_speakers}")

    # é‡‹æ”¾ Wav2Vec2ï¼Œç¯€çœ GPU è¨˜æ†¶é«”
    del w2v_model
    torch.cuda.empty_cache()

    train_dataset = TensorDataset(X_train, y_train, s_train)
    test_dataset  = TensorDataset(X_test,  y_test,  s_test)

    all_run_accs = []
    all_run_f1s  = []

    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Run {run_i} / {TOTAL_RUNS}")
        print(f"{'='*60}")

        run_seed = SEED + run_i - 1
        set_seed(run_seed)

        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)

        # è¨ˆç®— total_stepsï¼Œä¾› alpha global step æ¯”ä¾‹ä½¿ç”¨ï¼ˆå°é½Š DANN-FTï¼‰
        total_steps  = len(train_loader) * EPOCHS
        global_step  = 0

        print("ğŸ—ï¸ åˆå§‹åŒ–å…¨æ–° DANN æ¨¡å‹...")
        dann_model = DANN_Model(num_speakers=num_speakers).to(DEVICE)

        optimizer        = optim.Adam(dann_model.parameters(), lr=1e-3)
        criterion_class  = nn.CrossEntropyLoss()
        criterion_domain = nn.CrossEntropyLoss()

        # âœ… Best checkpoint è¿½è¹¤ï¼ˆä»¥ val F1 ç‚ºæº–ï¼‰
        best_val_f1      = -1.0
        best_model_state = None

        print("âš”ï¸ é–‹å§‹è¨“ç·´...")
        for epoch in range(EPOCHS):
            dann_model.train()
            total_loss = 0

            for inputs, labels, speakers in train_loader:
                inputs, labels, speakers = (
                    inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
                )

                # âœ… Alpha: global step æ¯”ä¾‹ï¼ˆå°é½Š DANN-FTï¼‰
                p     = global_step / max(total_steps, 1)
                alpha = 2.0 / (1.0 + np.exp(-10 * p)) - 1
                global_step += 1

                optimizer.zero_grad()
                class_out, domain_out = dann_model(inputs, alpha=alpha)
                loss = (criterion_class(class_out, labels)
                        + criterion_domain(domain_out, speakers))
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            # æ¯å€‹ epoch çµæŸå¾Œè©•ä¼°ï¼Œè¿½è¹¤ best checkpoint
            acc, f1, spk_acc = evaluate(dann_model, test_loader)

            # âœ… å„²å­˜æœ€ä½³æ¨¡å‹ç‹€æ…‹
            if f1 > best_val_f1:
                best_val_f1      = f1
                best_model_state = copy.deepcopy(dann_model.state_dict())

            if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:
                print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.2f} | "
                      f"Dep Acc: {acc:.4f} | F1: {f1:.4f} | "
                      f"Spk Acc: {spk_acc:.4f} | Best F1: {best_val_f1:.4f}")

        # âœ… è¼‰å…¥ best checkpoint è©•ä¼°æœ€çµ‚çµæœ
        print(f"\nğŸ† è¼‰å…¥ Best Checkpoint (F1={best_val_f1:.4f}) é€²è¡Œæœ€çµ‚è©•ä¼°...")
        dann_model.load_state_dict(best_model_state)
        final_acc, final_f1, final_spk_acc = evaluate(dann_model, test_loader)
        print(f"âœ… Run {run_i} æœ€çµ‚çµæœ â†’ Acc: {final_acc:.4f} | "
              f"F1: {final_f1:.4f} | Spk Acc: {final_spk_acc:.4f}")

        all_run_accs.append(final_acc)
        all_run_f1s.append(final_f1)

        # t-SNE
        print(f"\nğŸ¨ [Run {run_i}] ç¹ªè£½ t-SNE åœ–...")
        dann_feats, dann_spks = get_feats(dann_model, test_loader)
        tsne    = TSNE(n_components=2, random_state=42 + run_i, perplexity=30)
        feats2d = tsne.fit_transform(dann_feats)

        plt.figure(figsize=(10, 8))
        sns.scatterplot(
            x=feats2d[:, 0], y=feats2d[:, 1],
            hue=dann_spks, palette="tab10", legend=False
        )
        plt.title(f"DANN Feature Space (Scenario A) - Run {run_i}", fontsize=16)
        filename = f"tsne_dann_A_run_{run_i}.png"
        plt.savefig(filename)
        plt.close()
        print(f"âœ… åœ–ç‰‡å·²å„²å­˜: {filename}")

    # å½™ç¸½
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {TOTAL_RUNS} æ¬¡å¯¦é©—çµæœçµ±è¨ˆ (DANN Static Backbone â€” Scenario A v3)")
    print(f"{'='*60}")
    accs = np.array(all_run_accs)
    f1s  = np.array(all_run_f1s)
    for i, (a, f) in enumerate(zip(accs, f1s), 1):
        print(f"  Run {i}: Acc={a:.4f}, F1={f:.4f}")
    print(f"{'â”€'*40}")
    print(f"  å¹³å‡ Acc : {accs.mean():.4f} Â± {accs.std():.4f}")
    print(f"  å¹³å‡ F1  : {f1s.mean():.4f} Â± {f1s.std():.4f}")
