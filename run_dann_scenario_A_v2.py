"""
æ–°ç‰ˆ File 4 â€” DANN Scenario A (Screening / No Speaker Overlap)
==============================================================
ç›®æ¨™ï¼šèˆ‡ File 5 (run_dann.py) å®Œå…¨ä¸€è‡´çš„ DANN è¨“ç·´æ–¹æ³•è«–
å”¯ä¸€å·®ç•°ï¼šè³‡æ–™è·¯å¾‘ç‚º scenario_A_screening

èˆ‡ run_dann.py (File 5) çš„å·®ç•°æ¸…å–®ï¼š
  â†’ TRAIN_CSV_PATH: scenario_B_monitoring â†’ scenario_A_screening
  â†’ TEST_CSV_PATH:  scenario_B_monitoring â†’ scenario_A_screening
  â†’ t-SNE å­˜æª”åï¼štsne_dann_run_{i}.png â†’ tsne_dann_A_run_{i}.png
"""

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Function
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import torchaudio
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# ğŸ”§ 1. è¨­å®šå€ (Config) â€” å”¯ä¸€å·®ç•°ï¼šè³‡æ–™è·¯å¾‘
# ==========================================
TRAIN_CSV_PATH = "./experiment_sisman_scientific/scenario_A_screening/train.csv"
TEST_CSV_PATH = "./experiment_sisman_scientific/scenario_A_screening/test.csv"
AUDIO_ROOT = "" 

MODEL_NAME = "facebook/wav2vec2-base"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32
EPOCHS = 30  
TOTAL_RUNS = 5  # ğŸ”¥ è¨­å®šç¸½å…±è¦è·‘å¹¾æ¬¡

print(f"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {DEVICE}")

# ==========================================
# ğŸ§  2. æ¨¡å‹å®šç¾© (DANN Architecture)
# ==========================================
class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha
        return output, None

class GradientReversalLayer(nn.Module):
    def __init__(self):
        super(GradientReversalLayer, self).__init__()
    def forward(self, x, alpha=1.0):
        return GradientReversalFn.apply(x, alpha)

class DANN_Model(nn.Module):
    def __init__(self, input_dim=768, hidden_dim=128, num_classes=2, num_speakers=38):
        super(DANN_Model, self).__init__()
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        self.class_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_speakers)
        )
        self.grl = GradientReversalLayer()

    def forward(self, x, alpha=1.0):
        features = self.shared_encoder(x)
        class_output = self.class_classifier(features)
        reverse_features = self.grl(features, alpha)
        domain_output = self.domain_classifier(reverse_features)
        return class_output, domain_output

# ==========================================
# ğŸ“‚ 3. è³‡æ–™è™•ç†å·¥å…· (Data Utils)
# ==========================================
def extract_speaker_id(filepath):
    filename = os.path.basename(filepath)
    speaker_id = filename.split('_')[0] 
    return speaker_id

def prepare_data(csv_path, processor, model, speaker_to_idx=None, is_train=True):
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ æ­£åœ¨è™•ç† {csv_path} (å…± {len(df)} ç­†)...")
    
    features_list = []
    labels_list = []
    speaker_indices_list = []
    
    label_map = {'dep': 1, '1': 1, 1: 1, 'non': 0, '0': 0, 0: 0}

    if is_train and speaker_to_idx is None:
        all_speakers = df['path'].apply(extract_speaker_id).unique()
        all_speakers = sorted(all_speakers)
        speaker_to_idx = {spk: idx for idx, spk in enumerate(all_speakers)}
        print(f"ğŸ” [è¨“ç·´é›†] Speaker Map: {list(speaker_to_idx.items())[:5]}...")
    
    model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Extracting Features"):
            wav_path = os.path.join(AUDIO_ROOT, row['path'])
            try:
                waveform, sample_rate = torchaudio.load(wav_path)
                if sample_rate != 16000:
                    waveform = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)
                if waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0, keepdim=True)
                
                raw_label = str(row['label']).strip().lower()
                if raw_label in label_map:
                    final_label = label_map[raw_label]
                else:
                    continue

                inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors="pt", padding=True)
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu()
                
                features_list.append(embeddings)
                labels_list.append(final_label)
                
                spk_str = extract_speaker_id(wav_path)
                speaker_indices_list.append(speaker_to_idx.get(spk_str, 0))
                
            except Exception as e:
                print(f"âš ï¸ Error: {wav_path} -> {e}")
                continue

    if len(features_list) == 0:
        raise ValueError("âŒ éŒ¯èª¤ï¼šæ²’æœ‰ä»»ä½•è³‡æ–™è¢«æˆåŠŸè®€å–ï¼")

    X = torch.cat(features_list, dim=0)
    y = torch.tensor(labels_list, dtype=torch.long)
    s = torch.tensor(speaker_indices_list, dtype=torch.long)
    return X, y, s, speaker_to_idx

# è¼”åŠ©å‡½å¼ï¼šç”¨ä¾†å¾ DANN æ¨¡å‹æŠ½å–ç‰¹å¾µç•«åœ–
def get_feats(model, loader):
    model.eval()
    feats = []
    spks = []
    with torch.no_grad():
        for inputs, labels, speakers in loader:
            inputs = inputs.to(DEVICE)
            f = model.shared_encoder(inputs).cpu().numpy()
            feats.append(f)
            spks.extend(speakers.cpu().numpy())
    return np.vstack(feats), np.array(spks)

# ==========================================
# ğŸš€ 4. ä¸»ç¨‹å¼åŸ·è¡Œ
# ==========================================
if __name__ == "__main__":
    # --- A. æº–å‚™ç‰¹å¾µæå–å™¨ (å‡çµç‰ˆ) ---
    print("ğŸ§  è¼‰å…¥ Wav2Vec2 æ¨¡å‹...")
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    w2v_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)
    
    # --- B. æº–å‚™è³‡æ–™ (åªåšä¸€æ¬¡ï¼) ---
    print("\nğŸ“¦ æ­£åœ¨æº–å‚™è³‡æ–™ (ç‰¹å¾µæå–åªæœƒåŸ·è¡Œä¸€æ¬¡)...")
    X_train, y_train, s_train, speaker_map = prepare_data(TRAIN_CSV_PATH, processor, w2v_model, is_train=True)
    X_test, y_test, s_test, _ = prepare_data(TEST_CSV_PATH, processor, w2v_model, speaker_to_idx=speaker_map, is_train=False)
    
    num_speakers = len(speaker_map)
    
    # å»ºç«‹ Dataset (Tensor ä¸æœƒè®Šï¼ŒLoader åœ¨è¿´åœˆå…§é‡å»ºå³å¯)
    train_dataset = TensorDataset(X_train, y_train, s_train)
    test_dataset = TensorDataset(X_test, y_test, s_test)

    # --- C. åˆå§‹åŒ–çµæœæ”¶é›†å®¹å™¨ ---
    all_run_accs = []
    all_run_f1s = []

    # --- D. é–‹å§‹ 5 æ¬¡å¯¦é©—è¿´åœˆ ---
    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ é–‹å§‹ç¬¬ {run_i} / {TOTAL_RUNS} æ¬¡å¯¦é©— (Run {run_i})")
        print(f"{'='*60}")
        
        # 1. æ¯æ¬¡éƒ½è¦é‡æ–°å»ºç«‹ Loader (Shuffle ç¢ºä¿éš¨æ©Ÿæ€§)
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)
        
        # 2. æ¯æ¬¡éƒ½è¦é‡æ–°åˆå§‹åŒ–æ¨¡å‹ (ç¢ºä¿æ¬Šé‡é‡ç½®)
        print(f"ğŸ—ï¸ åˆå§‹åŒ–å…¨æ–°çš„ DANN æ¨¡å‹...")
        dann_model = DANN_Model(num_speakers=num_speakers).to(DEVICE)
        
        optimizer = optim.Adam(dann_model.parameters(), lr=0.001)
        criterion_class = nn.CrossEntropyLoss()
        criterion_domain = nn.CrossEntropyLoss()
        
        # 3. è¨“ç·´è¿´åœˆ
        print("âš”ï¸ é–‹å§‹è¨“ç·´...")
        for epoch in range(EPOCHS):
            dann_model.train()
            total_loss = 0
            
            # å‹•æ…‹èª¿æ•´ alpha
            p = float(epoch) / EPOCHS
            alpha = 2.0 / (1.0 + np.exp(-10 * p)) - 1
            
            for inputs, labels, speakers in train_loader:
                inputs, labels, speakers = inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
                
                optimizer.zero_grad()
                class_out, domain_out = dann_model(inputs, alpha=alpha)
                loss = criterion_class(class_out, labels) + criterion_domain(domain_out, speakers)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            # æ¯å€‹ Epoch ç°¡å–®è©•ä¼°ä¸€ä¸‹
            if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:
                dann_model.eval()
                correct_speakers = 0
                all_preds = []
                all_labels = []
                total_samples = 0
                with torch.no_grad():
                    for inputs, labels, speakers in test_loader:
                        inputs, labels, speakers = inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
                        class_out, domain_out = dann_model(inputs, alpha=0)
                        
                        _, preds = torch.max(class_out, 1)
                        all_preds.extend(preds.cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())
                        
                        _, spk_preds = torch.max(domain_out, 1)
                        correct_speakers += (spk_preds == speakers).sum().item()
                        total_samples += labels.size(0)
                
                acc = accuracy_score(all_labels, all_preds)
                f1 = f1_score(all_labels, all_preds, average='macro')  # âœ… ä¿®æ­£ï¼šè£œä¸Š f1 è¨ˆç®—
                spk_acc = correct_speakers / total_samples
                print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.2f} | Dep Acc: {acc:.4f} | Dep F1: {f1:.4f} | Spk Acc: {spk_acc:.4f}")

                # ğŸ”¥ æœ€å¾Œä¸€å€‹ epoch æ‰æ”¶é›†çµæœ
                if epoch == EPOCHS - 1:
                    all_run_accs.append(acc)
                    all_run_f1s.append(f1)

        # 4. ç•«åœ– (t-SNE) - å­˜æˆä¸åŒçš„æª”å
        print(f"\nğŸ¨ [Run {run_i}] æ­£åœ¨ç¹ªè£½ t-SNE åœ–...")
        dann_feats, dann_spks = get_feats(dann_model, test_loader)
        
        tsne = TSNE(n_components=2, random_state=42 + run_i, perplexity=30)
        feats_2d = tsne.fit_transform(dann_feats)
        
        plt.figure(figsize=(10, 8))
        limit = None
        sns.scatterplot(x=feats_2d[:limit,0], y=feats_2d[:limit,1], hue=dann_spks[:limit], palette="tab10", legend=False)
        plt.title(f"DANN Feature Space (Scenario A) - Run {run_i}", fontsize=16)
        
        filename = f"tsne_dann_A_run_{run_i}.png"
        plt.savefig(filename)
        plt.close()
        print(f"âœ… åœ–ç‰‡å·²å„²å­˜: {filename}")

    # --- E. 5 æ¬¡å¯¦é©—ç¸½çµ ---
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {TOTAL_RUNS} æ¬¡å¯¦é©—çµæœçµ±è¨ˆ (Scenario A)")
    print(f"{'='*60}")
    accs = np.array(all_run_accs)
    f1s = np.array(all_run_f1s)
    for i, (a, f) in enumerate(zip(accs, f1s), 1):
        print(f"  Run {i}: Acc={a:.4f}, F1={f:.4f}")
    print(f"{'â”€'*40}")
    print(f"  å¹³å‡ Acc : {accs.mean():.4f} Â± {accs.std():.4f}")
    print(f"  å¹³å‡ F1  : {f1s.mean():.4f} Â± {f1s.std():.4f}")
