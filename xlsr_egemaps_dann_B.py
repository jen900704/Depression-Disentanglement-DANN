"""
File â€” XLS-R + eGeMAPS + DANNï¼ˆç„¡ fine-tuneï¼ŒScenario Bï¼‰
=========================================================
æ¶æ§‹èªªæ˜ï¼š
  - XLS-R (wav2vec2-xls-r-300m) ä¸»å¹¹ï¼šå®Œå…¨å‡çµ
  - mean pooling å–æœ€å¾Œä¸€å±¤ hidden state â†’ 1024 ç¶­
  - eGeMAPS (opensmile, eGeMAPSv02 Functionals)ï¼š88 ç¶­
  - ç‰¹å¾µæ‹¼æ¥å¾Œ â†’ down_proj â†’ dep_classifier (binary)
  - DANNï¼šGRL + spk_classifierï¼Œalpha å‹•æ…‹éå¢

ä¿®æ­£æ¸…å–®ï¼ˆç›¸å°æ–¼ä½¿ç”¨è€…æä¾›çš„è‰ç¨¿ï¼‰ï¼š
  1. import èªæ³•åˆ†è¡Œ
  2. GradientReversalFn å®šç¾©ä½ç½®ç§»è‡³ model ä½¿ç”¨å‰
  3. forward å›å‚³ SpeechClassifierOutput (ModelOutput)ï¼ŒTrainer æ‰èƒ½æå– loss
  4. egemaps_feat å­˜å…¥ dataset æ¬„ä½ï¼ŒDataCollator è² è²¬çµ„ batch
  5. preprocess_function è£œå®Œé‡æ¡æ¨£é‚è¼¯ï¼›opensmile æ”¹ç”¨ process_signal é¿å…è·¯å¾‘é‡è¤‡è®€å–
  6. speaker_labels å¾æª”åæŠ½å–å¾Œå­˜å…¥ dataset
  7. AUDIO_ROOT / TOTAL_RUNS ç­‰è¨­å®šè£œå…¨
  8. alpha å‹•æ…‹æ³¨å…¥ CTCTrainerï¼ˆèˆ‡ SLS+DANN ç‰ˆä¸€è‡´ï¼‰
  9. è£œå®Œ TOTAL_RUNS è¿´åœˆ + è·¨ run çµ±è¨ˆ
 10. config è¨­å®š hidden_size å°æ‡‰ XLS-R çš„ 1024
 11. Wav2Vec2Processor â†’ Wav2Vec2FeatureExtractorï¼ˆXLS-R ç„¡ CTC tokenizerï¼‰
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchaudio
import opensmile
import matplotlib.pyplot as plt

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, List, Union, Any
from math import sqrt, exp

from torch.autograd import Function
from datasets import Dataset as HFDataset
from transformers import (
    Wav2Vec2FeatureExtractor,   # XLS-R ç„¡ CTC tokenizerï¼Œæ”¹ç”¨ FeatureExtractor
    Wav2Vec2Config,
    Wav2Vec2PreTrainedModel,
    Wav2Vec2Model,
    Trainer,
    TrainingArguments,
    EvalPrediction,
    set_seed,
)
from transformers.file_utils import ModelOutput
from packaging import version
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    mean_squared_error,
)

# ============================================================
#  è¨­å®šå€
# ============================================================
TRAIN_CSV  = "./experiment_sisman_scientific/scenario_B_monitoring/train.csv"
TEST_CSV   = "./experiment_sisman_scientific/scenario_B_monitoring/test.csv"
AUDIO_ROOT = "/export/fs05/hyeh10/depression/daic_5utt_full/merged_5"

MODEL_NAME  = "facebook/wav2vec2-xls-r-300m"
OUTPUT_DIR  = "./output_xlsr_egemaps_dann_B"
EGEMAPS_DIM = 88   # eGeMAPSv02 Functionals å›ºå®š 88 ç¶­

SEED       = 42
TOTAL_RUNS = 5
NUM_EPOCHS = 10
LEARNING_RATE = 1e-4
BATCH_SIZE    = 4
GRAD_ACCUM    = 2
EVAL_STEPS    = 50
SAVE_STEPS    = 50
LOGGING_STEPS = 50
SAVE_TOTAL_LIMIT = 2
FP16 = torch.cuda.is_available()

LABEL_MAP   = {"non": 0, "0": 0, 0: 0, "dep": 1, "1": 1, 1: 1}
LABEL_NAMES = ["non-depressed", "depressed"]
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# opensmile åˆå§‹åŒ–ï¼ˆç¨‹å¼å•Ÿå‹•æ™‚å»ºç«‹ä¸€æ¬¡ï¼‰
SMILE = opensmile.Smile(
    feature_set=opensmile.FeatureSet.eGeMAPSv02,
    feature_level=opensmile.FeatureLevel.Functionals,
)


# ============================================================
#  ModelOutput
# ============================================================

@dataclass
class SpeechClassifierOutput(ModelOutput):
    # ä¿®æ­£ 3ï¼šå›å‚³ ModelOutput å­é¡ï¼ŒTrainer æ‰èƒ½æ­£ç¢ºæå– loss / logits
    loss:           Optional[torch.FloatTensor] = None
    logits:         torch.FloatTensor           = None
    speaker_logits: Optional[torch.FloatTensor] = None
    hidden_states:  Optional[Tuple[torch.FloatTensor]] = None
    attentions:     Optional[Tuple[torch.FloatTensor]] = None


# ============================================================
#  GRLï¼ˆä¿®æ­£ 2ï¼šå®šç¾©åœ¨ Model ä¹‹å‰ï¼‰
# ============================================================

class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None


# ============================================================
#  æ¨¡å‹å®šç¾©
# ============================================================

class XLSR_eGeMaps_DANN(Wav2Vec2PreTrainedModel):
    """
    XLS-R (frozen) + eGeMAPS concat + DANN
    - xlsr mean pooling: 1024 ç¶­
    - eGeMAPS functionals: 88 ç¶­
    - concat â†’ 1112 ç¶­ â†’ down_proj(256) â†’ dep/spk classifier
    - alpha ç”± CTCTrainer å‹•æ…‹æ³¨å…¥ï¼ˆself._alphaï¼‰
    """
    def __init__(self, config, egemaps_dim: int = EGEMAPS_DIM):
        super().__init__(config)
        self.wav2vec2 = Wav2Vec2Model(config)

        # å‡çµ XLS-R å…¨éƒ¨åƒæ•¸
        for param in self.wav2vec2.parameters():
            param.requires_grad = False

        combined_dim = config.hidden_size + egemaps_dim  # 1024 + 88 = 1112
        self.down_proj = nn.Sequential(
            nn.Linear(combined_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
        )
        self.dep_classifier = nn.Linear(256, config.num_labels)
        self.spk_classifier = nn.Linear(256, getattr(config, "num_speakers", 38))

        self._alpha = 0.0  # ç”± CTCTrainer å‹•æ…‹æ›´æ–°
        self.init_weights()

    def freeze_feature_extractor(self):
        pass  # å…¨éƒ¨å·²å‡çµï¼Œä¿ç•™ä»‹é¢ç›¸å®¹æ€§

    def forward(
        self,
        input_values,
        attention_mask=None,
        egemaps_feat=None,       # ä¿®æ­£ 4ï¼šç”± dataset æ¬„ä½å‚³å…¥
        labels=None,
        speaker_labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            return_dict=True,
        )
        # XLS-R mean pooling over time axis â†’ [B, 1024]
        xlsr_feat = torch.mean(outputs.last_hidden_state, dim=1)

        # eGeMAPS ç‰¹å¾µæ‹¼æ¥ï¼ˆä¿®æ­£ï¼šç¢ºä¿ dtype ä¸€è‡´ï¼‰
        if egemaps_feat is not None:
            egemaps_feat = egemaps_feat.to(xlsr_feat.dtype)
            combined = torch.cat([xlsr_feat, egemaps_feat], dim=-1)  # [B, 1112]
        else:
            # æ¨è«–æ™‚è‹¥ç„¡ eGeMAPSï¼Œè£œé›¶ï¼ˆä¸å½±éŸ¿è¨“ç·´æµç¨‹ï¼‰
            zero_pad = torch.zeros(
                xlsr_feat.size(0), EGEMAPS_DIM,
                dtype=xlsr_feat.dtype, device=xlsr_feat.device
            )
            combined = torch.cat([xlsr_feat, zero_pad], dim=-1)

        shared     = self.down_proj(combined)                               # [B, 256]
        dep_logits = self.dep_classifier(shared)                            # [B, 2]
        spk_logits = self.spk_classifier(
            GradientReversalFn.apply(shared, self._alpha)
        )                                                                   # [B, num_speakers]

        loss = None
        if labels is not None and speaker_labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            dep_loss = loss_fct(dep_logits, labels)
            spk_loss = loss_fct(spk_logits, speaker_labels)
            loss = dep_loss + self._alpha * spk_loss

        return SpeechClassifierOutput(
            loss=loss,
            logits=dep_logits,
            speaker_logits=spk_logits,
        )


# ============================================================
#  DataCollatorï¼ˆå« egemaps_feat + speaker_labelsï¼‰
# ============================================================

@dataclass
class DataCollatorWithEGeMAPSAndSpeaker:
    processor: Wav2Vec2FeatureExtractor
    padding: Union[bool, str] = True

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_features   = [{"input_values": f["input_values"]} for f in features]
        label_features   = [f["labels"]          for f in features]
        speaker_features = [f["speaker_labels"]  for f in features]
        egemaps_features = [f["egemaps_feat"]    for f in features]

        # Wav2Vec2FeatureExtractor.pad èˆ‡ Wav2Vec2Processor.pad ä»‹é¢å®Œå…¨ç›¸åŒ
        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            return_tensors="pt",
        )
        batch["labels"]         = torch.tensor(label_features,   dtype=torch.long)
        batch["speaker_labels"] = torch.tensor(speaker_features, dtype=torch.long)
        batch["egemaps_feat"]   = torch.tensor(
            np.stack(egemaps_features), dtype=torch.float32
        )
        return batch


# ============================================================
#  compute_metrics
# ============================================================

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.argmax(preds, axis=1)
    acc = accuracy_score(p.label_ids, preds)
    f1  = f1_score(p.label_ids, preds, average="macro")
    return {"accuracy": acc, "f1": f1}


# ============================================================
#  CTCTrainer â€” å‹•æ…‹æ³¨å…¥ alphaï¼ˆä¿®æ­£ 8ï¼‰
# ============================================================

if version.parse(torch.__version__) >= version.parse("1.6"):
    from torch.cuda.amp import autocast


class CTCTrainer(Trainer):
    def training_step(
        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]
    ) -> torch.Tensor:
        total_steps = self.args.max_steps if self.args.max_steps > 0 else (
            len(self.train_dataset) // (
                self.args.per_device_train_batch_size * self.args.gradient_accumulation_steps
            ) * int(self.args.num_train_epochs)
        )
        p     = float(self.state.global_step) / max(total_steps, 1)
        alpha = 2.0 / (1.0 + exp(-10.0 * p)) - 1.0
        model._alpha = alpha

        model.train()
        inputs = self._prepare_inputs(inputs)

        is_amp_used = self.args.fp16 or self.args.bf16
        if is_amp_used:
            with torch.amp.autocast("cuda"):
                loss = self.compute_loss(model, inputs)
        else:
            loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps

        if is_amp_used:
            if hasattr(self, "scaler") and self.scaler is not None:
                self.scaler.scale(loss).backward()
            elif hasattr(self, "accelerator"):
                self.accelerator.backward(loss)
            else:
                loss.backward()
        else:
            loss.backward()

        return loss.detach()


# ============================================================
#  è³‡æ–™è¼‰å…¥èˆ‡é è™•ç†
# ============================================================

def extract_speaker_id(filepath: str) -> str:
    return os.path.basename(filepath).split("_")[0]


def extract_egemaps(wav_path: str) -> np.ndarray:
    """
    ä¿®æ­£ 5ï¼šä½¿ç”¨ process_signal é¿å…è·¯å¾‘èˆ‡ opensmile æ ¼å¼å•é¡Œï¼Œ
    ä¸¦åš NaN å¡« 0 ä¿è­·ã€‚
    """
    try:
        waveform, sr = torchaudio.load(wav_path)
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        if sr != 16000:
            waveform = torchaudio.functional.resample(waveform, sr, 16000)
        # opensmile éœ€è¦ numpy 1D array
        signal = waveform.squeeze().numpy()
        feat = SMILE.process_signal(signal, 16000).values.flatten().astype(np.float32)
        feat = np.nan_to_num(feat, nan=0.0)  # NaN ä¿è­·
        return feat
    except Exception as e:
        print(f"âš ï¸ eGeMAPS æå–å¤±æ•—: {wav_path} â†’ {e}")
        return np.zeros(EGEMAPS_DIM, dtype=np.float32)


def load_audio_dataset(csv_path: str, speaker_to_idx: dict = None, is_train: bool = True):
    """
    è¼‰å…¥ CSVï¼Œå›å‚³ HFDatasetï¼ˆå« egemaps_feat + speaker_labelsï¼‰åŠ speaker_to_idxã€‚
    ä¿®æ­£ 4 & 6ï¼šegemaps_feat / speaker_labels åœ¨æ­¤é å…ˆè¨ˆç®—ä¸¦å­˜å…¥ datasetã€‚
    """
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ è®€å– {csv_path}ï¼Œå…± {len(df)} ç­†è³‡æ–™")

    if is_train and speaker_to_idx is None:
        all_speakers  = sorted(set(extract_speaker_id(p) for p in df["path"].tolist()))
        speaker_to_idx = {spk: idx for idx, spk in enumerate(all_speakers)}
        print(f"ğŸ” åµæ¸¬åˆ° {len(speaker_to_idx)} ä½ speaker")

    records = []
    skipped = 0
    for _, row in df.iterrows():
        wav_path  = os.path.join(AUDIO_ROOT, row["path"])
        raw_label = str(row["label"]).strip().lower()

        if raw_label not in LABEL_MAP:
            skipped += 1
            continue
        if not os.path.exists(wav_path):
            print(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {wav_path}")
            skipped += 1
            continue

        spk_str = extract_speaker_id(wav_path)
        spk_idx = speaker_to_idx.get(spk_str, 0)

        records.append({
            "path":           wav_path,
            "label":          LABEL_MAP[raw_label],
            "speaker_labels": spk_idx,
        })

    if skipped:
        print(f"âš ï¸ è·³é {skipped} ç­†ç„¡æ•ˆè³‡æ–™")
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(records)} ç­†è³‡æ–™")

    dataset = HFDataset.from_dict({
        "path":           [r["path"]           for r in records],
        "label":          [r["label"]          for r in records],
        "speaker_labels": [r["speaker_labels"] for r in records],
    })
    return dataset, speaker_to_idx


def speech_file_to_array_fn(batch, processor):
    """è®€å–éŸ³è¨Šã€é‡æ¡æ¨£ï¼ŒåŒæ™‚æå– eGeMAPSï¼ˆä¿®æ­£ 5ï¼šæ•´åˆåœ¨åŒä¸€ map passï¼‰"""
    wav_path = batch["path"]

    # è®€å–æ³¢å½¢
    speech_array, sampling_rate = torchaudio.load(wav_path)
    if speech_array.shape[0] > 1:
        speech_array = torch.mean(speech_array, dim=0, keepdim=True)
    speech_array = speech_array.squeeze().numpy()

    if sampling_rate != 16000:
        import librosa
        speech_array = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=16000)

    batch["speech"]       = speech_array
    batch["egemaps_feat"] = extract_egemaps(wav_path)  # ä¿®æ­£ 5ï¼šåŒæ­¥æå–
    return batch


def preprocess_function(batch, processor):
    """å°‡ speech array è½‰ç‚º input_valuesï¼Œä¿ç•™ egemaps_feat"""
    result = processor(
        batch["speech"],
        sampling_rate=16000,
        return_tensors="np",
        padding=False,
    )
    batch["input_values"] = result.input_values[0]
    batch["labels"]       = batch["label"]
    # egemaps_feat å·²åœ¨ speech_file_to_array_fn å­˜å…¥ï¼Œæ­¤è™•ç›´æ¥ä¿ç•™
    return batch


def split_train_valid(dataset: HFDataset, valid_ratio: float = 0.15, seed: int = 42):
    split = dataset.train_test_split(test_size=valid_ratio, seed=seed)
    return split["train"], split["test"]


# ============================================================
#  è©•ä¼°èˆ‡å ±å‘Š
# ============================================================

def full_evaluation(trainer, test_dataset, output_dir, run_i):
    predictions = trainer.predict(test_dataset)
    preds  = predictions.predictions
    if isinstance(preds, tuple):
        preds = preds[0]
    y_pred = np.argmax(preds, axis=1)
    y_true = predictions.label_ids

    print("\n" + "=" * 60)
    print(f"ğŸ“Š [Run {run_i}] Classification Report")
    print("=" * 60)
    report    = classification_report(y_true, y_pred, target_names=LABEL_NAMES,
                                      zero_division=0, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    print(report_df)

    cm    = confusion_matrix(y_true, y_pred)
    cm_df = pd.DataFrame(cm, index=LABEL_NAMES, columns=LABEL_NAMES)
    print("\nğŸ“Š Confusion Matrix:")
    print(cm_df)

    mse  = mean_squared_error(y_true, y_pred)
    rmse = sqrt(mse)
    report_df["MSE"]  = mse
    report_df["RMSE"] = rmse

    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc     = auc(fpr, tpr)

    results_path = os.path.join(output_dir, f"results_run{run_i}")
    os.makedirs(results_path, exist_ok=True)
    report_df.to_csv(os.path.join(results_path, "clsf_report.csv"), sep="\t")
    cm_df.to_csv(    os.path.join(results_path, "conf_matrix.csv"), sep="\t")

    plt.figure()
    plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC (AUC={roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
    plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - Scenario B (XLS-R+eGeMAPS+DANN) Run {run_i}")
    plt.legend(loc="lower right")
    plt.savefig(os.path.join(results_path, "roc_curve.png"))
    plt.close()

    acc = accuracy_score(y_true, y_pred)
    f1  = f1_score(y_true, y_pred, average="macro")
    print(f"\nğŸ¯ Test Accuracy: {acc:.4f} | F1 (macro): {f1:.4f} | AUC: {roc_auc:.4f}")
    print(f"âœ… çµæœå·²å„²å­˜è‡³ {results_path}")
    return {"accuracy": acc, "f1": f1, "auc": roc_auc}


# ============================================================
#  ä¸»ç¨‹å¼ â€” TOTAL_RUNS æ¬¡è¿´åœˆï¼ˆä¿®æ­£ 9ï¼‰
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ XLS-R + eGeMAPS + DANNï¼ˆç„¡ fine-tuneï¼‰â€” Scenario B")
    print(f"   XLS-Rï¼šå…¨éƒ¨å‡çµ ({MODEL_NAME})")
    print(f"   eGeMAPSï¼š{EGEMAPS_DIM} ç¶­ (eGeMAPSv02 Functionals)")
    print("   DANN alphaï¼šå‹•æ…‹éå¢")
    print("=" * 60)

    set_seed(SEED)
    # XLS-R æ˜¯ç´”èªéŸ³è¡¨ç¤ºæ¨¡å‹ï¼Œæ²’æœ‰ CTC tokenizerï¼Œå¿…é ˆç”¨ FeatureExtractor
    processor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)

    # â”€â”€ è³‡æ–™èˆ‡ç‰¹å¾µæå–åªåšä¸€æ¬¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“¦ è¼‰å…¥è³‡æ–™é›†ï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    train_dataset_full, speaker_to_idx = load_audio_dataset(TRAIN_CSV, is_train=True)
    test_dataset_raw, _                = load_audio_dataset(
        TEST_CSV, speaker_to_idx=speaker_to_idx, is_train=False
    )
    num_speakers = len(speaker_to_idx)
    print(f"ğŸ‘¥ å…± {num_speakers} ä½ speaker")

    print("\nğŸ”Š é è™•ç†éŸ³è¨Š + æå– eGeMAPSï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼Œè€—æ™‚è¼ƒé•·ï¼‰...")
    train_dataset_full = train_dataset_full.map(
        speech_file_to_array_fn, fn_kwargs={"processor": processor}
    )
    test_dataset_raw = test_dataset_raw.map(
        speech_file_to_array_fn, fn_kwargs={"processor": processor}
    )
    train_dataset_full = train_dataset_full.map(
        preprocess_function, fn_kwargs={"processor": processor}
    )
    test_dataset = test_dataset_raw.map(
        preprocess_function, fn_kwargs={"processor": processor}
    )

    # â”€â”€ TOTAL_RUNS æ¬¡å¯¦é©—è¿´åœˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_results = []
    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Run {run_i} / {TOTAL_RUNS}")
        print(f"{'='*60}")

        set_seed(SEED + run_i)

        train_dataset, eval_dataset = split_train_valid(
            train_dataset_full, valid_ratio=0.15, seed=SEED + run_i
        )
        print(f"ğŸ“Š Train: {len(train_dataset)} | Valid: {len(eval_dataset)} | Test: {len(test_dataset)}")

        # æ¯æ¬¡é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¿®æ­£ 10ï¼šhidden_size XLS-R ç‚º 1024ï¼‰
        config = Wav2Vec2Config.from_pretrained(
            MODEL_NAME,
            num_labels=2,
            num_speakers=num_speakers,
            final_dropout=0.1,
        )
        model = XLSR_eGeMaps_DANN.from_pretrained(MODEL_NAME, config=config)

        frozen    = sum(1 for p in model.wav2vec2.parameters() if not p.requires_grad)
        total     = sum(1 for p in model.wav2vec2.parameters())
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"â„ï¸  wav2vec2 å‡çµ: {frozen}/{total} å€‹åƒæ•¸çµ„")
        print(f"ğŸ”¥ å¯è¨“ç·´åƒæ•¸ç¸½é‡: {trainable:,}")

        data_collator  = DataCollatorWithEGeMAPSAndSpeaker(processor=processor, padding=True)
        run_output_dir = os.path.join(OUTPUT_DIR, f"run_{run_i}")
        os.makedirs(run_output_dir, exist_ok=True)

        training_args = TrainingArguments(
            output_dir=run_output_dir,
            per_device_train_batch_size=BATCH_SIZE,
            per_device_eval_batch_size=BATCH_SIZE,
            gradient_accumulation_steps=GRAD_ACCUM,
            evaluation_strategy="steps",
            num_train_epochs=NUM_EPOCHS,
            fp16=FP16,
            save_steps=SAVE_STEPS,
            eval_steps=EVAL_STEPS,
            logging_steps=LOGGING_STEPS,
            learning_rate=LEARNING_RATE,
            save_total_limit=SAVE_TOTAL_LIMIT,
            seed=SEED + run_i,
            data_seed=SEED + run_i,
            load_best_model_at_end=True,
            # metric_for_best_model æœªè¨­å®š â†’ é è¨­ç”¨ validation loss
            report_to="none",
        )

        trainer = CTCTrainer(
            model=model,
            data_collator=data_collator,
            args=training_args,
            compute_metrics=compute_metrics,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=processor,   # FeatureExtractor æœ¬èº«å³æ˜¯ tokenizer åƒæ•¸
        )

        print("âš”ï¸ é–‹å§‹è¨“ç·´...")
        try:
            trainer.train()
        except RuntimeError as e:
            if "out of memory" in str(e):
                print("âš ï¸ GPU è¨˜æ†¶é«”ä¸è¶³ï¼Œæ¸…é™¤å¿«å–å¾Œè·³éæ­¤ run")
                torch.cuda.empty_cache()
                continue
            raise e

        best_path = os.path.join(run_output_dir, "best_model")
        trainer.save_model(best_path)
        processor.save_pretrained(best_path)   # FeatureExtractor æ”¯æ´ save_pretrained
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å„²å­˜è‡³: {best_path}")

        results = full_evaluation(trainer, test_dataset, OUTPUT_DIR, run_i)
        all_results.append(results)

    # â”€â”€ è·¨ run çµ±è¨ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if all_results:
        print("\n" + "=" * 60)
        print("ğŸ“ˆ è·¨ Run çµ±è¨ˆæ‘˜è¦")
        print("=" * 60)
        for metric in ["accuracy", "f1", "auc"]:
            vals = [r[metric] for r in all_results]
            print(f"  {metric.upper():10s}  mean={np.mean(vals):.4f}  std={np.std(vals):.4f}  "
                  f"min={np.min(vals):.4f}  max={np.max(vals):.4f}")

    print("\nğŸ Scenario B å¯¦é©—å®Œæˆï¼")
