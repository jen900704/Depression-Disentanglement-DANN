"""
DANN (Static Backbone) â€” Scenario B
=====================================
ä¿®æ­£ç‰ˆ v4ï¼šä¿®æ­£ Speaker Map åªåŒ…å« Target Groupï¼ˆ38ä½ï¼‰

ä¿®æ­£é …ç›®ï¼š
  [v4 æ–°å¢] Speaker Map åªå¾ test.csv å»ºç«‹ï¼ˆ38ä½ Target Groupï¼‰
            train ä¸­çš„è·¯äºº speaker index è¨­ç‚º -1ï¼Œä¸åƒèˆ‡ L_spk
            â†’ å°é½Šå¯¦é©—è¨­è¨ˆï¼šGRL åªé€¼æ¨¡å‹å¿˜æ‰ Target Group çš„è²ç´‹

  [v3 ä¿ç•™] Alpha schedule: global step æ¯”ä¾‹
  [v3 ä¿ç•™] Best checkpoint è¿½è¹¤ï¼ˆä»¥ val F1 ç‚ºæº–ï¼‰
  [v3 ä¿ç•™] çµæœå– best checkpoint

ä¸è®Šé …ç›®ï¼ˆèˆ‡ DANN-FT çš„å”¯ä¸€æ¶æ§‹å·®ç•°ï¼‰ï¼š
  - Wav2Vec2 å®Œå…¨å‡çµï¼ˆCNN + Transformer éƒ½ä¸æ›´æ–°ï¼‰
  - ç‰¹å¾µä¸€æ¬¡é æå–ï¼Œå­˜æˆ Tensorï¼ˆfrozen backbone ç„¡éœ€ backwardï¼‰
  - Optimizer: Adam, lr=1e-3, batch=32, epochs=30
"""

import os
import copy
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Function
from transformers import Wav2Vec2Processor, Wav2Vec2Model, set_seed
import torchaudio
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# ============================================================
#  è¨­å®šå€
# ============================================================
TRAIN_CSV_PATH = "./experiment_sisman_scientific/scenario_B_monitoring/train.csv"
TEST_CSV_PATH  = "./experiment_sisman_scientific/scenario_B_monitoring/test.csv"
AUDIO_ROOT     = ""

MODEL_NAME  = "facebook/wav2vec2-base"
DEVICE      = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE  = 32
EPOCHS      = 30
TOTAL_RUNS  = 5
SEED        = 103

print(f"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {DEVICE}")

# ============================================================
#  GRL
# ============================================================
class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None

class GradientReversalLayer(nn.Module):
    def forward(self, x, alpha=1.0):
        return GradientReversalFn.apply(x, alpha)

# ============================================================
#  æ¨¡å‹å®šç¾©
# ============================================================
class DANN_Model(nn.Module):
    def __init__(self, input_dim=768, hidden_dim=128, num_classes=2, num_speakers=38):
        super().__init__()
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
        )
        self.class_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes),
        )
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_speakers),
        )
        self.grl = GradientReversalLayer()

    def forward(self, x, alpha=1.0):
        features   = self.shared_encoder(x)
        class_out  = self.class_classifier(features)
        rev        = self.grl(features, alpha)
        domain_out = self.domain_classifier(rev)
        return class_out, domain_out

# ============================================================
#  Helper
# ============================================================
def extract_speaker_id(filepath):
    return os.path.basename(str(filepath)).split('_')[0]

# ============================================================
#  [v4] å»ºç«‹ Speaker Map â€” åªå¾ Target Groupï¼ˆtest.csvï¼‰å»ºç«‹
#  train ä¸­çš„è·¯äºº speaker index = -1ï¼ˆä¸åƒèˆ‡ L_spkï¼‰
# ============================================================
def build_speaker_map(test_csv_path):
    """
    åªç”¨ test.csv çš„ speaker å»ºç«‹ mapã€‚
    test set å…¨éƒ¨éƒ½æ˜¯ Target Groupï¼ˆ38ä½ï¼‰ï¼Œ
    train ä¸­ä¹Ÿæœ‰é€™ 38 ä½çš„æ­·å²éŒ„éŸ³ï¼Œ
    è·¯äººä¸åœ¨æ­¤ map å…§ï¼Œindex æœƒå›å‚³ -1ã€‚
    """
    df_test = pd.read_csv(test_csv_path)
    target_speakers = sorted(df_test['path'].apply(extract_speaker_id).unique())
    speaker_map = {spk: idx for idx, spk in enumerate(target_speakers)}
    print(f"ğŸ” [v4] Speaker Map åªåŒ…å« Target Group: {len(speaker_map)} ä½")
    return speaker_map

# ============================================================
#  è³‡æ–™è™•ç†
# ============================================================
def prepare_data(csv_path, processor, w2v_model, speaker_to_idx):
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ æ­£åœ¨è™•ç† {csv_path} (å…± {len(df)} ç­†)...")

    features_list, labels_list, speaker_indices_list = [], [], []
    label_map = {'dep': 1, '1': 1, 1: 1, 'non': 0, '0': 0, 0: 0}

    w2v_model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Extracting Features"):
            wav_path  = os.path.join(AUDIO_ROOT, row['path'])
            raw_label = str(row['label']).strip().lower()
            if raw_label not in label_map:
                continue
            try:
                waveform, sr = torchaudio.load(wav_path)
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)
                if sr != 16000:
                    waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)

                inputs = processor(
                    waveform.squeeze().numpy(),
                    sampling_rate=16000, return_tensors="pt", padding=True
                )
                inputs     = {k: v.to(DEVICE) for k, v in inputs.items()}
                embeddings = w2v_model(**inputs).last_hidden_state.mean(dim=1).cpu()

                features_list.append(embeddings)
                labels_list.append(label_map[raw_label])

                # [v4] è·¯äººä¸åœ¨ speaker_map è£¡ â†’ index = -1
                spk_id = extract_speaker_id(wav_path)
                speaker_indices_list.append(speaker_to_idx.get(spk_id, -1))

            except Exception as e:
                print(f"âš ï¸ Error: {wav_path} -> {e}")

    if not features_list:
        raise ValueError("âŒ æ²’æœ‰ä»»ä½•è³‡æ–™è¢«æˆåŠŸè®€å–ï¼")

    X = torch.cat(features_list, dim=0)
    y = torch.tensor(labels_list,          dtype=torch.long)
    s = torch.tensor(speaker_indices_list, dtype=torch.long)

    n_target   = (s >= 0).sum().item()
    n_stranger = (s < 0).sum().item()
    print(f"   â†’ Target Group: {n_target} ç­† | è·¯äºº (s=-1): {n_stranger} ç­†")

    return X, y, s

# ============================================================
#  è©•ä¼°å‡½å¼
#  Spk Acc åªåœ¨ test setï¼ˆå…¨æ˜¯ Target Groupï¼Œs >= 0ï¼‰è¨ˆç®—
# ============================================================
def evaluate(model, loader):
    model.eval()
    all_preds, all_labels = [], []
    correct_spk, total_spk = 0, 0
    with torch.no_grad():
        for inputs, labels, speakers in loader:
            inputs, labels, speakers = (
                inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
            )
            class_out, domain_out = model(inputs, alpha=0.0)
            _, preds     = torch.max(class_out,  1)
            _, spk_preds = torch.max(domain_out, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            # test set å…¨æ˜¯ Target Groupï¼Œs å…¨ >= 0
            mask = speakers >= 0
            if mask.sum() > 0:
                correct_spk += (spk_preds[mask] == speakers[mask]).sum().item()
                total_spk   += mask.sum().item()

    acc     = accuracy_score(all_labels, all_preds)
    f1      = f1_score(all_labels, all_preds, average='macro')
    spk_acc = correct_spk / total_spk if total_spk > 0 else 0.0
    return acc, f1, spk_acc

# t-SNE ç”¨ç‰¹å¾µæŠ½å–
def get_feats(model, loader):
    model.eval()
    feats, spks = [], []
    with torch.no_grad():
        for inputs, labels, speakers in loader:
            inputs = inputs.to(DEVICE)
            f = model.shared_encoder(inputs).cpu().numpy()
            feats.append(f)
            spks.extend(speakers.cpu().numpy())
    return np.vstack(feats), np.array(spks)

# ============================================================
#  ä¸»ç¨‹å¼
# ============================================================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ DANN (Static Backbone) â€” Scenario B  [v4 ä¿®æ­£ç‰ˆ]")
    print("   Speaker Map: åªå« Target Group (38ä½ï¼Œå¾ test.csv å»ºç«‹)")
    print("   è·¯äºº speaker index = -1ï¼Œä¸åƒèˆ‡ L_spk")
    print("   Alpha: global step æ¯”ä¾‹ | Best checkpoint: val F1")
    print(f"   LR=1e-3 | Batch={BATCH_SIZE} | Epochs={EPOCHS} | Runs={TOTAL_RUNS}")
    print("=" * 60)

    # [v4] å…ˆå¾ test.csv å»ºç«‹æ­£ç¢ºçš„ Speaker Mapï¼ˆ38ä½ Target Groupï¼‰
    speaker_map  = build_speaker_map(TEST_CSV_PATH)
    num_speakers = len(speaker_map)
    print(f"âœ… num_speakers = {num_speakers}ï¼ˆæ‡‰ç‚º 38ï¼‰")

    # ç‰¹å¾µæå–ï¼ˆåªåšä¸€æ¬¡ï¼‰
    print("\nğŸ§  è¼‰å…¥ Wav2Vec2ï¼ˆå‡çµï¼‰...")
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    w2v_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)

    print("\nğŸ“¦ é æå–ç‰¹å¾µï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    X_train, y_train, s_train = prepare_data(
        TRAIN_CSV_PATH, processor, w2v_model, speaker_map
    )
    X_test, y_test, s_test = prepare_data(
        TEST_CSV_PATH, processor, w2v_model, speaker_map
    )
    print(f"âœ… Train: {len(X_train)} ç­† | Test: {len(X_test)} ç­†")

    # é‡‹æ”¾ Wav2Vec2ï¼Œç¯€çœ GPU è¨˜æ†¶é«”
    del w2v_model
    torch.cuda.empty_cache()

    train_dataset = TensorDataset(X_train, y_train, s_train)
    test_dataset  = TensorDataset(X_test,  y_test,  s_test)

    all_run_accs = []
    all_run_f1s  = []

    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Run {run_i} / {TOTAL_RUNS}")
        print(f"{'='*60}")

        run_seed = SEED + run_i - 1
        set_seed(run_seed)

        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)

        total_steps = len(train_loader) * EPOCHS
        global_step = 0

        print("ğŸ—ï¸ åˆå§‹åŒ–å…¨æ–° DANN æ¨¡å‹...")
        dann_model = DANN_Model(num_speakers=num_speakers).to(DEVICE)

        optimizer        = optim.Adam(dann_model.parameters(), lr=1e-3)
        criterion_class  = nn.CrossEntropyLoss()
        criterion_domain = nn.CrossEntropyLoss()

        best_val_f1      = -1.0
        best_model_state = None

        print("âš”ï¸ é–‹å§‹è¨“ç·´...")
        for epoch in range(EPOCHS):
            dann_model.train()
            total_loss       = 0
            total_loss_dep   = 0
            total_loss_spk   = 0

            for inputs, labels, speakers in train_loader:
                inputs, labels, speakers = (
                    inputs.to(DEVICE), labels.to(DEVICE), speakers.to(DEVICE)
                )

                p     = global_step / max(total_steps, 1)
                alpha = 2.0 / (1.0 + np.exp(-10 * p)) - 1
                global_step += 1

                optimizer.zero_grad()
                class_out, domain_out = dann_model(inputs, alpha=alpha)

                loss_dep = criterion_class(class_out, labels)

                # [v4] åªå° Target Groupï¼ˆs >= 0ï¼‰è¨ˆç®— L_spk
                mask = speakers >= 0
                if mask.sum() > 0:
                    loss_spk = criterion_domain(domain_out[mask], speakers[mask])
                else:
                    loss_spk = torch.tensor(0.0, device=DEVICE)

                loss = loss_dep + loss_spk
                loss.backward()
                optimizer.step()

                total_loss     += loss.item()
                total_loss_dep += loss_dep.item()
                total_loss_spk += loss_spk.item()

            # Epoch çµæŸå¾Œè©•ä¼°
            acc, f1, spk_acc = evaluate(dann_model, test_loader)

            if f1 > best_val_f1:
                best_val_f1      = f1
                best_model_state = copy.deepcopy(dann_model.state_dict())

            if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:
                print(f"Epoch {epoch+1}/{EPOCHS} | "
                      f"Loss: {total_loss:.2f} (dep={total_loss_dep:.2f}, spk={total_loss_spk:.2f}) | "
                      f"Dep Acc: {acc:.4f} | F1: {f1:.4f} | "
                      f"Spk Acc: {spk_acc:.4f} | Best F1: {best_val_f1:.4f}")

        # è¼‰å…¥ best checkpoint
        print(f"\nğŸ† è¼‰å…¥ Best Checkpoint (F1={best_val_f1:.4f}) é€²è¡Œæœ€çµ‚è©•ä¼°...")
        dann_model.load_state_dict(best_model_state)
        final_acc, final_f1, final_spk_acc = evaluate(dann_model, test_loader)
        print(f"âœ… Run {run_i} æœ€çµ‚çµæœ â†’ Acc: {final_acc:.4f} | "
              f"F1: {final_f1:.4f} | Spk Acc: {final_spk_acc:.4f}")

        all_run_accs.append(final_acc)
        all_run_f1s.append(final_f1)

        # t-SNE
        print(f"\nğŸ¨ [Run {run_i}] ç¹ªè£½ t-SNE åœ–...")
        dann_feats, dann_spks = get_feats(dann_model, test_loader)
        tsne    = TSNE(n_components=2, random_state=42 + run_i, perplexity=30)
        feats2d = tsne.fit_transform(dann_feats)

        plt.figure(figsize=(10, 8))
        sns.scatterplot(
            x=feats2d[:, 0], y=feats2d[:, 1],
            hue=dann_spks, palette="tab10", legend=False
        )
        plt.title(f"DANN (Static) Feature Space â€” Scenario B, Run {run_i}", fontsize=16)
        filename = f"tsne_dann_B_run_{run_i}.png"
        plt.savefig(filename)
        plt.close()
        print(f"âœ… åœ–ç‰‡å·²å„²å­˜: {filename}")

    # å½™ç¸½
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {TOTAL_RUNS} æ¬¡å¯¦é©—çµæœçµ±è¨ˆ (DANN Static Backbone â€” v4)")
    print(f"{'='*60}")
    accs = np.array(all_run_accs)
    f1s  = np.array(all_run_f1s)
    for i, (a, f) in enumerate(zip(accs, f1s), 1):
        print(f"  Run {i}: Acc={a:.4f}, F1={f:.4f}")
    print(f"{'â”€'*40}")
    print(f"  å¹³å‡ Acc : {accs.mean():.4f} Â± {accs.std():.4f}")
    print(f"  å¹³å‡ F1  : {f1s.mean():.4f} Â± {f1s.std():.4f}")
