"""
File â€” Huang + SLS + DANNï¼ˆç„¡ fine-tuneï¼ŒScenario Aï¼‰
=====================================================
æ¶æ§‹èªªæ˜ï¼š
  - Wav2Vec2 ä¸»å¹¹ï¼šå®Œå…¨å‡çµï¼ˆåŒ…å« CNN feature extractor + Transformer encoderï¼‰
  - SLS (Stochastic Layer Selection)ï¼šå¯å­¸ç¿’çš„åŠ æ¬Šèåˆæ‰€æœ‰ hidden states
  - DANN domain classifierï¼šå°æŠ—å¼è¨“ç·´ï¼Œä»¥ speaker ç‚º domain
  - dep_classifierï¼šäºŒå…ƒæ†‚é¬±åˆ†é¡ (binary)

èˆ‡ replicate_huang (File 2/3) çš„å·®ç•°ï¼š
  â†’ wav2vec2 å…¨éƒ¨å‡çµï¼ˆéåªå‡çµ CNNï¼‰ï¼Œåªæœ‰ SLS æ¬Šé‡ + å…©å€‹åˆ†é¡é ­å¯è¨“ç·´
  â†’ åŠ å…¥ GRL + speaker domain classifier
  â†’ åŸ·è¡Œ TOTAL_RUNS æ¬¡ï¼Œæ¯æ¬¡é‡æ–°åˆå§‹åŒ– SLS/åˆ†é¡é ­æ¬Šé‡

ä¿®æ­£æ¸…å–®ï¼ˆç›¸å°æ–¼ä½¿ç”¨è€…æä¾›çš„è‰ç¨¿ï¼‰ï¼š
  1. import èªæ³•åˆ†è¡Œ
  2. forward å›å‚³ SpeechClassifierOutputï¼ˆModelOutput å­é¡ï¼‰ï¼ŒTrainer æ‰èƒ½æ­£ç¢ºæå– loss
  3. speaker_labels å¾ CSV æŠ½å–å¾Œå­˜å…¥ datasetï¼Œè®“ Trainer èƒ½å‚³å…¥ forward
  4. è£œä¸Š AUDIO_ROOT
  5. alpha å‹•æ…‹èª¿æ•´ï¼šåœ¨è‡ªè¨‚ CTCTrainer.training_step ä¸­ä¾å…¨å±€ step è¨ˆç®—
  6. config è¨­å®š output_hidden_states=True
  7. è£œå®Œ TOTAL_RUNS è¿´åœˆ
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchaudio
import matplotlib.pyplot as plt

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, List, Union, Any
from math import sqrt, exp

from torch.autograd import Function
from datasets import Dataset as HFDataset
from transformers import (
    Wav2Vec2Processor,
    Wav2Vec2Config,
    Wav2Vec2PreTrainedModel,
    Wav2Vec2Model,
    Trainer,
    TrainingArguments,
    EvalPrediction,
    set_seed,
)
from transformers.file_utils import ModelOutput
from packaging import version
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    mean_squared_error,
)

# ============================================================
#  è¨­å®šå€
# ============================================================
TRAIN_CSV  = "./experiment_sisman_scientific/scenario_A_screening/train.csv"
TEST_CSV   = "./experiment_sisman_scientific/scenario_A_screening/test.csv"
AUDIO_ROOT = "/export/fs05/hyeh10/depression/daic_5utt_full/merged_5"  # â† ä¿®æ­£ 1ï¼šè£œä¸Š AUDIO_ROOT

MODEL_NAME = "facebook/wav2vec2-base"
OUTPUT_DIR = "./output_huang_sls_dann_A"

SEED       = 42
TOTAL_RUNS = 5
NUM_EPOCHS = 10
LEARNING_RATE = 1e-4
BATCH_SIZE    = 4
GRAD_ACCUM    = 2
EVAL_STEPS    = 50
SAVE_STEPS    = 50
LOGGING_STEPS = 50
SAVE_TOTAL_LIMIT = 2
FP16 = torch.cuda.is_available()

LABEL_MAP   = {"non": 0, "0": 0, 0: 0, "dep": 1, "1": 1, 1: 1}
LABEL_NAMES = ["non-depressed", "depressed"]
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# ============================================================
#  æ¨¡å‹å®šç¾©
# ============================================================

@dataclass
class SpeechClassifierOutput(ModelOutput):
    # ä¿®æ­£ 2ï¼šå›å‚³ ModelOutput å­é¡ï¼ŒTrainer æ‰èƒ½æ­£ç¢ºæå– loss / logits
    loss:           Optional[torch.FloatTensor] = None
    logits:         torch.FloatTensor           = None
    speaker_logits: Optional[torch.FloatTensor] = None
    hidden_states:  Optional[Tuple[torch.FloatTensor]] = None
    attentions:     Optional[Tuple[torch.FloatTensor]] = None


class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None


class Wav2Vec2_SLS_DANN(Wav2Vec2PreTrainedModel):
    """
    Wav2Vec2 + SLS + DANNï¼Œç„¡ fine-tune
    - wav2vec2 å…¨éƒ¨å‡çµ
    - SLS åŠ æ¬Šèåˆæ‰€æœ‰ hidden statesï¼ˆ13 å±¤ï¼Œå« CNN embedding è¼¸å‡ºï¼‰
    - dep_classifierï¼šbinary classification
    - spk_classifierï¼šspeaker domain classifierï¼ˆé€é GRL å°æŠ—ï¼‰
    - alpha ç”±å¤–éƒ¨ CTCTrainer å‹•æ…‹æ³¨å…¥ï¼ˆå­˜æ–¼ self._alphaï¼‰
    """
    def __init__(self, config):
        super().__init__(config)
        self.wav2vec2 = Wav2Vec2Model(config)

        # å‡çµ wav2vec2 å…¨éƒ¨åƒæ•¸ï¼ˆå« CNN + Transformerï¼‰
        for param in self.wav2vec2.parameters():
            param.requires_grad = False

        num_layers = config.num_hidden_layers + 1  # +1 for CNN embedding output (layer 0)
        self.sls_weights = nn.Parameter(torch.ones(num_layers))

        self.down_proj = nn.Sequential(
            nn.Linear(config.hidden_size, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
        )
        self.dep_classifier = nn.Linear(128, config.num_labels)
        self.spk_classifier = nn.Linear(128, 200)

        # alpha ç”± CTCTrainer åœ¨æ¯å€‹ step å‰æ›´æ–°
        self._alpha = 0.0

        self.init_weights()

    def freeze_feature_extractor(self):
        # ç›¸å®¹ train.py å‘¼å«ç¿’æ…£ï¼Œæ­¤è™• wav2vec2 å·²å…¨å‡ï¼Œä¿ç•™æ­¤ä»‹é¢å³å¯
        pass

    def forward(
        self,
        input_values,
        attention_mask=None,
        labels=None,
        speaker_labels=None,  # ä¿®æ­£ 3ï¼šç”± dataset æ¬„ä½å‚³å…¥
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        # ä¿®æ­£ 6ï¼šoutput_hidden_states å¿…é ˆç‚º Trueï¼ŒSLS æ‰èƒ½å–åˆ°æ‰€æœ‰å±¤
        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_hidden_states=True,  # å¼·åˆ¶é–‹å•Ÿ
            output_attentions=output_attentions,
            return_dict=True,
        )

        # hidden_states: tuple of (num_layers+1) tensors, each [B, T, H]
        hidden_states = torch.stack(outputs.hidden_states)  # [L, B, T, H]
        weights = torch.softmax(self.sls_weights, dim=0)    # [L]
        fused = (hidden_states * weights.view(-1, 1, 1, 1)).sum(0)  # [B, T, H]

        # Mean pooling over time
        shared = self.down_proj(torch.mean(fused, dim=1))   # [B, 128]

        dep_logits = self.dep_classifier(shared)             # [B, num_labels]
        spk_logits = self.spk_classifier(
            GradientReversalFn.apply(shared, self._alpha)   # alpha å‹•æ…‹æ³¨å…¥
        )                                                    # [B, num_speakers]

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            dep_loss = loss_fct(dep_logits, labels)
            loss = dep_loss
        if speaker_labels is not None:
            mask = speaker_labels >= 0  # éæ¿¾ test çš„é™Œç”Ÿäººï¼ˆ-1ï¼‰
            if mask.sum() > 0:
                spk_loss = nn.CrossEntropyLoss()(
                    spk_logits[mask].view(-1, spk_logits.size(-1)),
                    speaker_labels[mask].view(-1)
                )
                loss = loss + self._alpha * spk_loss if loss is not None else spk_loss

        return SpeechClassifierOutput(
            loss=loss,
            logits=dep_logits,
            speaker_logits=spk_logits,
            hidden_states=None,
            attentions=None,
        )


# ============================================================
#  DataCollatorï¼ˆå« speaker_labelsï¼‰
# ============================================================

@dataclass
class DataCollatorWithSpeaker:
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_features = [{"input_values": f["input_values"]} for f in features]
        label_features   = [f["labels"]          for f in features]
        speaker_features = [f["speaker_labels"]  for f in features]

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            return_tensors="pt",
        )
        batch["labels"]         = torch.tensor(label_features,   dtype=torch.long)
        batch["speaker_labels"] = torch.tensor(speaker_features, dtype=torch.long)
        return batch


# ============================================================
#  compute_metrics
# ============================================================

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.argmax(preds, axis=1)
    acc = accuracy_score(p.label_ids, preds)
    f1  = f1_score(p.label_ids, preds, average="macro")
    return {"accuracy": acc, "f1": f1}


# ============================================================
#  CTCTrainer â€” ä¿®æ­£ 5ï¼šå‹•æ…‹æ³¨å…¥ alpha
# ============================================================

if version.parse(torch.__version__) >= version.parse("1.6"):
    from torch.cuda.amp import autocast


class CTCTrainer(Trainer):
    """
    åœ¨æ¯å€‹ training_step å‰ï¼Œä¾å…¨å±€è¨“ç·´é€²åº¦å‹•æ…‹æ›´æ–° model._alphaã€‚
    alpha å…¬å¼èˆ‡ run_dann.py ä¸€è‡´ï¼šalpha = 2/(1+exp(-10*p)) - 1ï¼Œp âˆˆ [0,1]
    """
    def training_step(
        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]
    ) -> torch.Tensor:

        # è¨ˆç®—ç•¶å‰è¨“ç·´é€²åº¦ p âˆˆ [0, 1]
        total_steps = self.args.max_steps if self.args.max_steps > 0 else (
            len(self.train_dataset) // (
                self.args.per_device_train_batch_size * self.args.gradient_accumulation_steps
            ) * int(self.args.num_train_epochs)
        )
        current_step = self.state.global_step
        p = float(current_step) / max(total_steps, 1)
        alpha = 2.0 / (1.0 + exp(-10.0 * p)) - 1.0

        # æ³¨å…¥ alpha åˆ°æ¨¡å‹
        model._alpha = alpha

        model.train()
        inputs = self._prepare_inputs(inputs)

        is_amp_used = self.args.fp16 or self.args.bf16
        if is_amp_used:
            with torch.amp.autocast("cuda"):
                loss = self.compute_loss(model, inputs)
        else:
            loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps

        if is_amp_used:
            if hasattr(self, "scaler") and self.scaler is not None:
                self.scaler.scale(loss).backward()
            elif hasattr(self, "accelerator"):
                self.accelerator.backward(loss)
            else:
                loss.backward()
        else:
            loss.backward()

        return loss.detach()


# ============================================================
#  è³‡æ–™è¼‰å…¥èˆ‡é è™•ç†
# ============================================================

def extract_speaker_id(filepath: str) -> str:
    return os.path.basename(filepath).split("_")[0]


def load_audio_dataset(csv_path: str, speaker_to_idx: dict = None, is_train: bool = True):
    """
    è¼‰å…¥ CSVï¼Œå›å‚³ HFDataset åŠï¼ˆè¨“ç·´æ™‚å»ºç«‹çš„ï¼‰speaker_to_idxã€‚
    ä¿®æ­£ 3ï¼šspeaker_labels å­˜å…¥ datasetï¼ŒTrainer å¯ä»¥ç›´æ¥å‚³çµ¦ forwardã€‚
    """
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ è®€å– {csv_path}ï¼Œå…± {len(df)} ç­†è³‡æ–™")

    # å»ºç«‹ speaker â†’ index å°ç…§è¡¨ï¼ˆåªåœ¨è¨“ç·´é›†å»ºç«‹ï¼‰
    if is_train and speaker_to_idx is None:
        all_wav_paths = df["path"].tolist()
        all_speakers  = sorted(set(extract_speaker_id(p) for p in all_wav_paths))
        speaker_to_idx = {spk: idx for idx, spk in enumerate(all_speakers)}
        print(f"ğŸ” åµæ¸¬åˆ° {len(speaker_to_idx)} ä½ speaker")

    records = []
    skipped = 0
    for _, row in df.iterrows():
        wav_path  = os.path.join(AUDIO_ROOT, row["path"])
        raw_label = str(row["label"]).strip().lower()

        if raw_label not in LABEL_MAP:
            skipped += 1
            continue
        if not os.path.exists(wav_path):
            print(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {wav_path}")
            skipped += 1
            continue

        spk_str = extract_speaker_id(wav_path)
        spk_idx = speaker_to_idx.get(spk_str, -1)  # é™Œç”Ÿäºº(test) â†’ -1ï¼Œforward è£¡ mask éæ¿¾

        records.append({
            "path":           wav_path,
            "label":          LABEL_MAP[raw_label],
            "speaker_labels": spk_idx,
        })

    if skipped:
        print(f"âš ï¸ è·³é {skipped} ç­†ç„¡æ•ˆ/ä¸å­˜åœ¨çš„è³‡æ–™")
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(records)} ç­†è³‡æ–™")

    dataset = HFDataset.from_dict({
        "path":           [r["path"]           for r in records],
        "label":          [r["label"]          for r in records],
        "speaker_labels": [r["speaker_labels"] for r in records],
    })
    return dataset, speaker_to_idx


def speech_file_to_array_fn(batch, processor):
    speech_array, sampling_rate = torchaudio.load(batch["path"])
    if speech_array.shape[0] > 1:
        speech_array = torch.mean(speech_array, dim=0, keepdim=True)
    speech_array = speech_array.squeeze().numpy()
    if sampling_rate != 16000:
        import librosa
        speech_array = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=16000)
    batch["speech"] = speech_array
    return batch


def preprocess_function(batch, processor):
    result = processor(batch["speech"], sampling_rate=16000, return_tensors="np", padding=False)
    batch["input_values"] = result.input_values[0]
    batch["labels"]       = batch["label"]
    return batch


def split_train_valid(dataset: HFDataset, valid_ratio: float = 0.15, seed: int = 42):
    split = dataset.train_test_split(test_size=valid_ratio, seed=seed)
    return split["train"], split["test"]


# ============================================================
#  è©•ä¼°èˆ‡å ±å‘Š
# ============================================================

def full_evaluation(trainer, test_dataset, output_dir, run_i):
    predictions = trainer.predict(test_dataset)
    preds  = predictions.predictions
    if isinstance(preds, tuple):
        preds = preds[0]
    y_pred = np.argmax(preds, axis=1)
    y_true = predictions.label_ids

    print("\n" + "=" * 60)
    print(f"ğŸ“Š [Run {run_i}] Classification Report")
    print("=" * 60)
    report = classification_report(y_true, y_pred, target_names=LABEL_NAMES,
                                   zero_division=0, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    print(report_df)

    cm     = confusion_matrix(y_true, y_pred)
    cm_df  = pd.DataFrame(cm, index=LABEL_NAMES, columns=LABEL_NAMES)
    print("\nğŸ“Š Confusion Matrix:")
    print(cm_df)

    mse  = mean_squared_error(y_true, y_pred)
    rmse = sqrt(mse)
    report_df["MSE"]  = mse
    report_df["RMSE"] = rmse

    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc     = auc(fpr, tpr)

    results_path = os.path.join(output_dir, f"results_run{run_i}")
    os.makedirs(results_path, exist_ok=True)

    report_df.to_csv(os.path.join(results_path, "clsf_report.csv"), sep="\t")
    cm_df.to_csv(    os.path.join(results_path, "conf_matrix.csv"), sep="\t")

    plt.figure()
    plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC (AUC={roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
    plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - Scenario A Run {run_i}")
    plt.legend(loc="lower right")
    plt.savefig(os.path.join(results_path, "roc_curve.png"))
    plt.close()

    acc = accuracy_score(y_true, y_pred)
    f1  = f1_score(y_true, y_pred, average="macro")
    print(f"\nğŸ¯ Test Accuracy: {acc:.4f} | F1 (macro): {f1:.4f} | AUC: {roc_auc:.4f}")
    print(f"âœ… çµæœå·²å„²å­˜è‡³ {results_path}")
    return {"accuracy": acc, "f1": f1, "auc": roc_auc}


# ============================================================
#  ä¸»ç¨‹å¼ â€” TOTAL_RUNS æ¬¡è¿´åœˆï¼ˆä¿®æ­£ 7ï¼‰
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Huang + SLS + DANNï¼ˆç„¡ fine-tuneï¼‰â€” Scenario A")
    print("   wav2vec2ï¼šå…¨éƒ¨å‡çµ")
    print("   SLS æ¬Šé‡ + dep/spk classifierï¼šå¯è¨“ç·´")
    print("   DANN alphaï¼šå‹•æ…‹éå¢")
    print("=" * 60)

    set_seed(SEED)
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)

    # â”€â”€ è³‡æ–™åªæº–å‚™ä¸€æ¬¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ“¦ è¼‰å…¥è³‡æ–™é›†ï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    train_dataset_full, speaker_to_idx = load_audio_dataset(
        TRAIN_CSV, is_train=True
    )
    test_dataset_raw, _ = load_audio_dataset(
        TEST_CSV, speaker_to_idx=speaker_to_idx, is_train=False
    )
    num_speakers = len(speaker_to_idx)
    print(f"ğŸ‘¥ å…± {num_speakers} ä½ speaker")

    print("\nğŸ”Š é è™•ç†éŸ³è¨Šï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    train_dataset_full = train_dataset_full.map(
        speech_file_to_array_fn, fn_kwargs={"processor": processor}
    )
    test_dataset_raw = test_dataset_raw.map(
        speech_file_to_array_fn, fn_kwargs={"processor": processor}
    )
    train_dataset_full = train_dataset_full.map(
        preprocess_function, fn_kwargs={"processor": processor}
    )
    test_dataset = test_dataset_raw.map(
        preprocess_function, fn_kwargs={"processor": processor}
    )

    # â”€â”€ TOTAL_RUNS æ¬¡å¯¦é©—è¿´åœˆï¼ˆä¿®æ­£ 7ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_results = []
    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Run {run_i} / {TOTAL_RUNS}")
        print(f"{'='*60}")

        set_seed(SEED + run_i)  # æ¯æ¬¡ run ç”¨ä¸åŒ seedï¼Œç¢ºä¿éš¨æ©Ÿæ€§

        train_dataset = train_dataset_full
        eval_dataset  = test_dataset
        print(f"ğŸ“Š Train: {len(train_dataset)} | Test(eval): {len(test_dataset)}")

        # æ¯æ¬¡é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¿®æ­£ 6ï¼šconfig è¨­å®š output_hidden_statesï¼‰
        config = Wav2Vec2Config.from_pretrained(
            MODEL_NAME,
            num_labels=2,
            num_speakers=num_speakers,
            final_dropout=0.1,
            output_hidden_states=True,   # ä¿®æ­£ 6ï¼šè®“ wav2vec2 è¼¸å‡ºæ‰€æœ‰ hidden states
        )
        model = Wav2Vec2_SLS_DANN.from_pretrained(MODEL_NAME, config=config)

        # ç¢ºèªå‡çµç‹€æ…‹
        frozen = sum(1 for p in model.wav2vec2.parameters() if not p.requires_grad)
        total  = sum(1 for p in model.wav2vec2.parameters())
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"â„ï¸  wav2vec2 å‡çµ: {frozen}/{total} å€‹åƒæ•¸çµ„")
        print(f"ğŸ”¥ å¯è¨“ç·´åƒæ•¸ç¸½é‡: {trainable:,}")

        data_collator = DataCollatorWithSpeaker(processor=processor, padding=True)

        run_output_dir = os.path.join(OUTPUT_DIR, f"run_{run_i}")
        os.makedirs(run_output_dir, exist_ok=True)

        training_args = TrainingArguments(
            output_dir=run_output_dir,
            per_device_train_batch_size=BATCH_SIZE,
            label_names=["labels"],
            dataloader_drop_last=True,
            per_device_eval_batch_size=BATCH_SIZE,
            gradient_accumulation_steps=GRAD_ACCUM,
            evaluation_strategy="steps",
            num_train_epochs=NUM_EPOCHS,
            fp16=FP16,
            save_steps=SAVE_STEPS,
            eval_steps=EVAL_STEPS,
            logging_steps=LOGGING_STEPS,
            learning_rate=LEARNING_RATE,
            save_total_limit=SAVE_TOTAL_LIMIT,
            seed=SEED + run_i,
            data_seed=SEED + run_i,
            load_best_model_at_end=True,
            # metric_for_best_model æœªè¨­å®š â†’ é è¨­ç”¨ validation lossï¼Œå°é½Š train.py
            report_to="none",
            remove_unused_columns=False,  # ğŸ”¥ é˜²æ­¢ Trainer åˆªæ‰ speaker_labels
        )

        trainer = CTCTrainer(
            model=model,
            data_collator=data_collator,
            args=training_args,
            compute_metrics=compute_metrics,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=processor.feature_extractor,
        )

        print("âš”ï¸ é–‹å§‹è¨“ç·´...")
        try:
            trainer.train()
        except RuntimeError as e:
            if "out of memory" in str(e):
                print("âš ï¸ GPU è¨˜æ†¶é«”ä¸è¶³ï¼Œæ¸…é™¤å¿«å–å¾Œè·³éæ­¤ run")
                torch.cuda.empty_cache()
                continue
            raise e

        # å„²å­˜æœ€ä½³æ¨¡å‹
        best_path = os.path.join(run_output_dir, "best_model")
        trainer.save_model(best_path)
        processor.save_pretrained(best_path)
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å„²å­˜è‡³: {best_path}")

        pth_path = os.path.join(OUTPUT_DIR, f"huang_sls_dann_A_shared_encoder_run_{run_i}.pth")
        torch.save(trainer.model.down_proj.state_dict(), pth_path)
        print(f"ğŸ”‘ down_proj .pth å„²å­˜è‡³: {pth_path}")

        # å®Œæ•´è©•ä¼°
        results = full_evaluation(trainer, test_dataset, OUTPUT_DIR, run_i)
        all_results.append(results)

    # â”€â”€ è·¨ run çµ±è¨ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if all_results:
        print("\n" + "=" * 60)
        print("ğŸ“ˆ è·¨ Run çµ±è¨ˆæ‘˜è¦")
        print("=" * 60)
        for metric in ["accuracy", "f1", "auc"]:
            vals = [r[metric] for r in all_results]
            print(f"  {metric.upper():10s}  mean={np.mean(vals):.4f}  std={np.std(vals):.4f}  "
                  f"min={np.min(vals):.4f}  max={np.max(vals):.4f}")

    print("\nğŸ Scenario A å¯¦é©—å®Œæˆï¼")