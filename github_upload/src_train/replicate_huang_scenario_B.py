"""
File 3 â€” Scenario B (Partial Speaker Overlap) â”€â”€ v3 é€Ÿåº¦ & OOM ä¿®æ­£ç‰ˆ
=======================================================================
å°é½Š File 6+7+8 (build_model.py / train.py / evaluate.py) çš„è¨“ç·´ç®¡ç·š
- æ¨¡å‹æ¶æ§‹ï¼šWav2Vec2ForSpeechClassification (mean pooling + frozen CNN)
- è¨“ç·´æ¡†æ¶ï¼šHuggingFace Trainer (AdamW + linear LR scheduler)
- checkpoint é¸æ“‡ï¼ševal_lossï¼ˆå°é½Š yaml é è¨­ï¼Œä¸æŒ‡å®š metric_for_best_modelï¼‰
- compute_metricsï¼šaccuracy onlyï¼ˆå°é½Š build_model.py File 6ï¼‰
- return_attention_maskï¼šFalseï¼ˆå°é½Š yamlï¼‰
- äº”æ¬¡å¯¦é©—è¿´åœˆï¼Œæœ€å¾Œè¼¸å‡ºå¹³å‡èˆ‡æ¨™æº–å·®

v3 ä¿®æ”¹æ‘˜è¦ï¼ˆé€Ÿåº¦ & OOM é˜²è­·ï¼‰ï¼š
  â‘  eval_steps / save_steps / logging_stepsï¼š10 â†’ 100ï¼ˆæ¸›å°‘ eval æ¬¡æ•¸ 10xï¼‰
  â‘¡ dataloader_num_workers=0 + pin_memory=Falseï¼šé¿å…å¤š worker ä½”ç”¨é¡å¤– RAM
  â‘¢ OOM æ•æ‰ï¼šwhole training loop + empty_cache + gc.collect()
  â‘£ æ¯æ¬¡ eval / predict å‰å¾ŒåŠ  torch.cuda.empty_cache() é˜²æ­¢è©•ä¼°æ™‚ OOM
  â‘¤ æ¯æ¬¡ run çµæŸ del model/trainer + empty_cacheï¼Œé˜²æ­¢ 5 æ¬¡å¯¦é©—ç´¯ç©è¨˜æ†¶é«”
  â‘¥ gradient_checkpointing ä¿ç•™ï¼ˆçœ GPU è¨˜æ†¶é«”ï¼Œä»£åƒ¹æ˜¯è¼•å¾®æ¸›é€Ÿï¼‰
  â‘¦ ä¿ç•™ fp16=Trueï¼ˆCUDA ç’°å¢ƒä¸‹çœè¨˜æ†¶é«”ä¸”åŠ é€Ÿï¼‰
  â‘§ LengthGroupedSamplerï¼šè¨“ç·´æ™‚æŒ‰éŸ³è¨Šé•·åº¦æ’åºï¼Œæ¸›å°‘ padding æµªè²» â†’ é˜² OOM
  â‘¨ CTCTrainer è¦†å¯« get_train_dataloader / get_eval_dataloader ä½¿ç”¨é•·åº¦æ’åº
  â€» éŸ³è¨Šé•·åº¦ä¸æˆªæ–·ï¼Œå®Œæ•´ä¿ç•™åŸå§‹è³‡æ–™

å°é½Š daic-c2-rmse-roc.yaml è¨­å®šï¼ˆä¸å‹•ï¼‰ï¼š
  seed=103 | lr=1e-5 | epochs=10 | batch=1+grad_accum=8 (eff=8)
  freeze_feature_extractor=True | pooling_mode=mean
  return_attention_mask=False | metric_for_best_model â†’ eval_loss (é è¨­)
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchaudio
import matplotlib.pyplot as plt

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, List, Union, Any
from math import sqrt

from datasets import Dataset as HFDataset
from transformers import (
    Wav2Vec2Processor,
    Wav2Vec2Config,
    Wav2Vec2PreTrainedModel,
    Wav2Vec2Model,
    Trainer,
    TrainingArguments,
    EvalPrediction,
    set_seed,
)
from transformers.file_utils import ModelOutput
from packaging import version

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    mean_squared_error,
)

# ============================================================
#  è¨­å®šå€ â€” è·¯å¾‘ç‚º scenario_B_monitoringï¼Œå…¶é¤˜å°é½Š yaml
# ============================================================
TRAIN_CSV  = "./experiment_sisman_scientific/scenario_B_monitoring/train.csv"
TEST_CSV   = "./experiment_sisman_scientific/scenario_B_monitoring/test.csv"
AUDIO_ROOT = "/export/fs05/hyeh10/depression/daic_5utt_full/merged_5"

MODEL_NAME = "facebook/wav2vec2-base"
OUTPUT_DIR = "./output_scenario_B_v2"

SEED                         = 103
NUM_EPOCHS                   = 10
LEARNING_RATE                = 1e-5
PER_DEVICE_TRAIN_BATCH_SIZE  = 1
PER_DEVICE_EVAL_BATCH_SIZE   = 1
GRADIENT_ACCUMULATION_STEPS  = 8
FP16                         = torch.cuda.is_available()
# â–¼ v3 ä¿®æ­£ï¼ševal é »ç‡é™ä½ 10 å€ï¼Œå¤§å¹…ç¸®çŸ­ç¸½è¨“ç·´æ™‚é–“
EVAL_STEPS                   = 100
SAVE_STEPS                   = 100
LOGGING_STEPS                = 50
SAVE_TOTAL_LIMIT             = 2

TOTAL_RUNS = 5  # äº”æ¬¡å¯¦é©—å–å¹³å‡

LABEL_MAP   = {"non": 0, "0": 0, 0: 0, "dep": 1, "1": 1, 1: 1}
LABEL_NAMES = ["non-depressed", "depressed"]

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# ============================================================
#  æ¨¡å‹å®šç¾© â€” å®Œå…¨å°æ‡‰ build_model.py (File 6)
# ============================================================

@dataclass
class SpeechClassifierOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None


class Wav2Vec2ClassificationHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense    = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout  = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, features, **kwargs):
        x = features
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels   = config.num_labels
        self.pooling_mode = getattr(config, "pooling_mode", "mean")
        self.config       = config

        self.wav2vec2   = Wav2Vec2Model(config)
        self.classifier = Wav2Vec2ClassificationHead(config)
        self.init_weights()

    def freeze_feature_extractor(self):
        self.wav2vec2.feature_extractor._freeze_parameters()

    def merged_strategy(self, hidden_states, mode="mean"):
        if mode == "mean":
            return torch.mean(hidden_states, dim=1)
        elif mode == "max":
            return torch.max(hidden_states, dim=1)[0]
        else:
            raise Exception("Pooling methods: 'mean', 'max'")

    def forward(
        self,
        input_values,
        attention_mask=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        labels=None,
    ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = outputs[0]
        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)
        logits = self.classifier(hidden_states)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels > 1 and (
                    labels.dtype == torch.long or labels.dtype == torch.int
                ):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "single_label_classification":
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = nn.BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SpeechClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


# ============================================================
#  DataCollator â€” å®Œå…¨å°æ‡‰ build_model.py (File 6)
# ============================================================

@dataclass
class DataCollatorCTCWithPadding:
    processor:                 Wav2Vec2Processor
    padding:                   Union[bool, str] = True
    max_length:                Optional[int] = None
    max_length_labels:         Optional[int] = None
    pad_to_multiple_of:        Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_features = [{"input_values": f["input_values"]} for f in features]
        label_features = [f["labels"] for f in features]

        d_type = torch.long if isinstance(label_features[0], int) else torch.float

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        batch["labels"] = torch.tensor(label_features, dtype=d_type)
        return batch


# ============================================================
#  compute_metrics â€” å°é½Š build_model.py (File 6)
#  å›å‚³ accuracy onlyï¼Œé‚è¼¯èˆ‡ File 6 ä¸€è‡´
# ============================================================

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.argmax(preds, axis=1)
    return {"accuracy": (preds == p.label_ids).astype(np.float32).mean().item()}


# ============================================================
#  CTCTrainer â€” å®Œå…¨å°æ‡‰ train.py (File 7)
# ============================================================

if version.parse(torch.__version__) >= version.parse("1.6"):
    _is_native_amp_available = True


def _length_sorted_indices(dataset) -> list:
    """
    å›å‚³æŒ‰ input_values é•·åº¦æ’åºçš„ index æ¸…å–®ã€‚
    é•·åº¦ç›¸è¿‘çš„æ¨£æœ¬æœƒè¢«æ’åœ¨ä¸€èµ·ï¼Œå¤§å¹…æ¸›å°‘ batch å…§ paddingï¼Œé˜²æ­¢ OOMã€‚
    """
    lengths = [len(dataset[i]["input_values"]) for i in range(len(dataset))]
    return sorted(range(len(lengths)), key=lambda i: lengths[i])


class CTCTrainer(Trainer):
    def training_step(
        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]
    ) -> torch.Tensor:
        model.train()
        inputs = self._prepare_inputs(inputs)

        is_amp_used = self.args.fp16 or self.args.bf16

        if is_amp_used:
            with torch.amp.autocast("cuda"):
                loss = self.compute_loss(model, inputs)
        else:
            loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps

        if is_amp_used:
            if hasattr(self, "scaler") and self.scaler is not None:
                self.scaler.scale(loss).backward()
            elif hasattr(self, "accelerator"):
                self.accelerator.backward(loss)
            else:
                loss.backward()
        else:
            loss.backward()

        return loss.detach()

    def get_train_dataloader(self):
        """
        â–¼ v3 OOM é˜²è­·ï¼šè¨“ç·´æ™‚æŒ‰éŸ³è¨Šé•·åº¦æ’åºï¼ˆé•·åº¦ç›¸è¿‘çš„æ’åœ¨ä¸€èµ·ï¼‰ï¼Œ
          å¤§å¹…æ¸›å°‘ batch å…§ padding å¤§å°ï¼Œæ˜¯ A100 ä¸Šæœ€æœ‰æ•ˆçš„éæˆªæ–· OOM å°ç­–ã€‚
          batch_size=1 æ™‚æ¯ç­†ç¨ç«‹ï¼Œpadding ç”± collator åšåˆ° batch å…§æœ€é•·ï¼Œ
          æ’åºå¾Œæœ€é•·éŸ³è¨Šä¸æœƒèˆ‡æœ€çŸ­æ··åœ¨åŒä¸€ batchï¼Œé¿å…æš´å¢çš„ padding tensorã€‚
        """
        from torch.utils.data import DataLoader, Subset, SequentialSampler
        dataset = self.train_dataset
        sorted_indices = _length_sorted_indices(dataset)
        sorted_dataset = Subset(dataset, sorted_indices)
        return DataLoader(
            sorted_dataset,
            batch_size=self.args.per_device_train_batch_size,
            sampler=SequentialSampler(sorted_dataset),
            collate_fn=self.data_collator,
            num_workers=self.args.dataloader_num_workers,
            pin_memory=self.args.dataloader_pin_memory,
        )

    def get_eval_dataloader(self, eval_dataset=None):
        """
        â–¼ v3 OOM é˜²è­·ï¼šè©•ä¼°æ™‚åŒæ¨£æŒ‰é•·åº¦æ’åºï¼Œé˜²æ­¢è¶…é•·éŸ³è¨Šåœ¨è©•ä¼°ä¸­è§¸ç™¼ OOMã€‚
        """
        from torch.utils.data import DataLoader, Subset, SequentialSampler
        dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
        sorted_indices = _length_sorted_indices(dataset)
        sorted_dataset = Subset(dataset, sorted_indices)
        return DataLoader(
            sorted_dataset,
            batch_size=self.args.per_device_eval_batch_size,
            sampler=SequentialSampler(sorted_dataset),
            collate_fn=self.data_collator,
            num_workers=self.args.dataloader_num_workers,
            pin_memory=self.args.dataloader_pin_memory,
        )


# ============================================================
#  è³‡æ–™è¼‰å…¥èˆ‡é è™•ç†
# ============================================================

def load_audio_dataset(csv_path: str) -> HFDataset:
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ è®€å– {csv_path}ï¼Œå…± {len(df)} ç­†è³‡æ–™")

    records, skipped = [], 0
    for _, row in df.iterrows():
        wav_path  = os.path.join(AUDIO_ROOT, row["path"])
        raw_label = str(row["label"]).strip().lower()

        if raw_label not in LABEL_MAP:
            skipped += 1
            continue
        if not os.path.exists(wav_path):
            print(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {wav_path}")
            skipped += 1
            continue

        records.append({"path": wav_path, "label": LABEL_MAP[raw_label]})

    if skipped > 0:
        print(f"âš ï¸ è·³é {skipped} ç­†ç„¡æ•ˆ/ä¸å­˜åœ¨çš„è³‡æ–™")
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(records)} ç­†è³‡æ–™")

    return HFDataset.from_dict({
        "path":  [r["path"]  for r in records],
        "label": [r["label"] for r in records],
    })


def speech_file_to_array_fn(batch, processor):
    speech_array, sampling_rate = torchaudio.load(batch["path"])

    if speech_array.shape[0] > 1:
        speech_array = torch.mean(speech_array, dim=0, keepdim=True)
    speech_array = speech_array.squeeze().numpy()

    if sampling_rate != 16000:
        import librosa
        speech_array = librosa.resample(
            speech_array, orig_sr=sampling_rate, target_sr=16000
        )

    batch["speech"] = speech_array
    return batch


def preprocess_function(batch, processor):
    """return_attention_mask=False â€” å°é½Š yaml"""
    result = processor(
        batch["speech"],
        sampling_rate=16000,
        return_tensors="np",
        padding=False,
        return_attention_mask=False,  # å°é½Š yaml: return_attention_mask: False
    )
    batch["input_values"] = result.input_values[0]
    batch["labels"]       = batch["label"]
    return batch


# ============================================================
#  è©•ä¼°èˆ‡å ±å‘Š â€” å°æ‡‰ evaluate.py (File 8)
# ============================================================

def full_evaluation(trainer, test_dataset, output_dir, run_i):
    predictions = trainer.predict(test_dataset)
    preds = predictions.predictions
    if isinstance(preds, tuple):
        preds = preds[0]
    y_pred = np.argmax(preds, axis=1)
    y_true = predictions.label_ids

    report    = classification_report(y_true, y_pred, target_names=LABEL_NAMES,
                                      zero_division=0, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    print(report_df)

    cm    = confusion_matrix(y_true, y_pred)
    cm_df = pd.DataFrame(cm, index=LABEL_NAMES, columns=LABEL_NAMES)
    print("\nğŸ“Š Confusion Matrix:")
    print(cm_df)

    mse  = mean_squared_error(y_true, y_pred)
    rmse = sqrt(mse)
    report_df["MSE"]  = mse
    report_df["RMSE"] = rmse

    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc     = auc(fpr, tpr)

    results_path = os.path.join(output_dir, f"results_run_{run_i}")
    os.makedirs(results_path, exist_ok=True)

    report_df.to_csv(os.path.join(results_path, "clsf_report.csv"), sep="\t")
    cm_df.to_csv(os.path.join(results_path,     "conf_matrix.csv"), sep="\t")

    plt.figure()
    plt.plot(fpr, tpr, color="darkorange", lw=2,
             label=f"ROC curve (AUC = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
    plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - Scenario B (Run {run_i})")
    plt.legend(loc="lower right")
    plt.savefig(os.path.join(results_path, "roc_curve.png"))
    plt.close()
    print(f"âœ… Run {run_i} çµæœå·²å„²å­˜è‡³ {results_path}")

    acc = accuracy_score(y_true, y_pred)
    f1  = f1_score(y_true, y_pred, average="macro")
    return {"accuracy": acc, "f1": f1, "auc": roc_auc}


# ============================================================
#  ä¸»è¨“ç·´æµç¨‹ â€” äº”æ¬¡è¿´åœˆå–å¹³å‡
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Scenario B â€” å°é½Š daic-c2-rmse-roc.yaml è¨“ç·´ç®¡ç·š")
    print("   æ¨¡å‹: Wav2Vec2ForSpeechClassification (mean pooling)")
    print("   CNN: Frozen | åˆ†é¡: Binary | seed: 103 | lr: 1e-5")
    print("   checkpoint é¸æ“‡: eval_loss (å°é½Š yaml é è¨­)")
    print(f"   å¯¦é©—æ¬¡æ•¸: {TOTAL_RUNS} æ¬¡ï¼Œæœ€å¾Œè¼¸å‡ºå¹³å‡èˆ‡æ¨™æº–å·®")
    print("=" * 60)

    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    map_kwargs = {"fn_kwargs": {"processor": processor}}

    # è³‡æ–™åªè¼‰å…¥ä¸€æ¬¡
    print("\nğŸ“¦ è¼‰å…¥ä¸¦é è™•ç†è³‡æ–™é›†ï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰...")
    train_dataset_raw = load_audio_dataset(TRAIN_CSV)
    test_dataset_raw  = load_audio_dataset(TEST_CSV)

    train_dataset_raw = train_dataset_raw.map(speech_file_to_array_fn, **map_kwargs)
    test_dataset_raw  = test_dataset_raw.map(speech_file_to_array_fn,  **map_kwargs)

    train_dataset = train_dataset_raw.map(preprocess_function, **map_kwargs)
    test_dataset  = test_dataset_raw.map(preprocess_function,  **map_kwargs)

    print(f"ğŸ“Š Train: {len(train_dataset)} ç­† | Test: {len(test_dataset)} ç­†")

    data_collator_obj = DataCollatorCTCWithPadding(processor=processor, padding=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    all_results = []

    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ é–‹å§‹ç¬¬ {run_i} / {TOTAL_RUNS} æ¬¡å¯¦é©—")
        print(f"{'='*60}")

        # æ¯æ¬¡ seed éå¢ç¢ºä¿éš¨æ©Ÿæ€§ï¼š103, 104, 105, 106, 107
        run_seed = SEED + run_i - 1
        set_seed(run_seed)
        print(f"ğŸ² Run {run_i} seed: {run_seed}")

        # æ¯æ¬¡é‡æ–°åˆå§‹åŒ–æ¨¡å‹
        config = Wav2Vec2Config.from_pretrained(
            MODEL_NAME,
            num_labels=2,
            final_dropout=0.1,
            pooling_mode="mean",   # å°é½Š yaml: pooling_mode: mean
        )
        model = Wav2Vec2ForSpeechClassification.from_pretrained(MODEL_NAME, config=config)
        model.freeze_feature_extractor()  # å°é½Š yaml: freeze_feature_extractor: True
        print(f"â„ï¸ Feature Extractor (CNN) å·²å‡çµ")

        run_output_dir = os.path.join(OUTPUT_DIR, f"run_{run_i}")
        os.makedirs(run_output_dir, exist_ok=True)

        training_args = TrainingArguments(
            output_dir=run_output_dir,
            per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
            per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,
            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
            evaluation_strategy="steps",
            num_train_epochs=NUM_EPOCHS,
            fp16=FP16,
            save_steps=SAVE_STEPS,
            eval_steps=EVAL_STEPS,
            logging_steps=LOGGING_STEPS,
            learning_rate=LEARNING_RATE,
            save_total_limit=SAVE_TOTAL_LIMIT,
            seed=run_seed,
            data_seed=run_seed,
            load_best_model_at_end=True,
            # metric_for_best_model ä¸è¨­å®š â†’ é è¨­ eval_lossï¼ˆå°é½Š yamlï¼‰
            report_to="none",
            gradient_checkpointing=True,
            # â–¼ v3ï¼šé—œé–‰å¤š workerï¼Œé¿å… DataLoader ä½”ç”¨é¡å¤– RAM
            dataloader_num_workers=0,
            dataloader_pin_memory=False,
        )

        trainer = CTCTrainer(
            model=model,
            data_collator=data_collator_obj,
            args=training_args,
            compute_metrics=compute_metrics,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,   # ç„¡ç¨ç«‹ valid setï¼Œå°é½Šè«–æ–‡åšæ³•
            tokenizer=processor.feature_extractor,
        )

        print("âš”ï¸ é–‹å§‹è¨“ç·´...")
        # â–¼ v3ï¼šå®Œæ•´ OOM é˜²è­· â€” æ•æ‰æ•´å€‹ train()ï¼Œæ¸… cache å¾Œæç¤º
        try:
            trainer.train()
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("\nâš ï¸  OOMï¼å˜—è©¦æ¸…é™¤ GPU cache å¾Œç¹¼çºŒè©•ä¼°ï¼ˆè¨“ç·´æœªå®Œæ•´å®Œæˆï¼‰...")
                torch.cuda.empty_cache()
                import gc; gc.collect()
            else:
                raise e

        best_model_path = os.path.join(run_output_dir, "best_model")
        trainer.save_model(best_model_path)
        processor.save_pretrained(best_model_path)
        print(f"ğŸ’¾ Run {run_i} æœ€ä½³æ¨¡å‹å·²å„²å­˜è‡³: {best_model_path}")

        # â–¼ v3ï¼šè©•ä¼°å‰é‡‹æ”¾ GPU è¨˜æ†¶é«”
        torch.cuda.empty_cache()
        import gc; gc.collect()
        print(f"\nğŸ“Š Run {run_i} æ¸¬è©¦é›†è©•ä¼°...")
        results = full_evaluation(trainer, test_dataset, OUTPUT_DIR, run_i)
        results["run"] = run_i
        all_results.append(results)
        print(f"Run {run_i} â†’ Acc: {results['accuracy']:.4f} | F1: {results['f1']:.4f} | AUC: {results['auc']:.4f}")

        # â–¼ v3ï¼šæ¯æ¬¡ run çµæŸå¾Œé‡‹æ”¾æ¨¡å‹è¨˜æ†¶é«”ï¼Œé¿å…å¤šæ¬¡å¯¦é©—ç´¯ç© OOM
        del model, trainer
        torch.cuda.empty_cache()
        import gc; gc.collect()

    # ============================================================
    #  è¼¸å‡ºäº”æ¬¡å¹³å‡èˆ‡æ¨™æº–å·®
    # ============================================================
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ Scenario B â€” {TOTAL_RUNS} æ¬¡å¯¦é©—å½™ç¸½")
    print(f"{'='*60}")

    results_df = pd.DataFrame(all_results)
    print(results_df.to_string(index=False))

    summary = {
        "accuracy_mean": results_df["accuracy"].mean(),
        "accuracy_std":  results_df["accuracy"].std(),
        "f1_mean":       results_df["f1"].mean(),
        "f1_std":        results_df["f1"].std(),
        "auc_mean":      results_df["auc"].mean(),
        "auc_std":       results_df["auc"].std(),
    }

    print(f"\nğŸ¯ Accuracy : {summary['accuracy_mean']:.4f} Â± {summary['accuracy_std']:.4f}")
    print(f"ğŸ¯ F1 (macro): {summary['f1_mean']:.4f} Â± {summary['f1_std']:.4f}")
    print(f"ğŸ“ˆ AUC       : {summary['auc_mean']:.4f} Â± {summary['auc_std']:.4f}")

    summary_path = os.path.join(OUTPUT_DIR, "summary_5runs.csv")
    results_df.to_csv(summary_path, index=False)
    print(f"\nâœ… å½™ç¸½çµæœå·²å„²å­˜è‡³ {summary_path}")
    print("\nğŸ Scenario B å…¨éƒ¨å¯¦é©—å®Œæˆï¼")
