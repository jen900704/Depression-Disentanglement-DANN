"""
æ–°ç‰ˆ File 2 â€” Scenario A (Strict Speaker Split)
==============================================
ç›®æ¨™ï¼šä½¿ç”¨èˆ‡ HXS572 (6+7+8) å®Œå…¨ä¸€è‡´çš„è¨“ç·´æ–¹æ³•è«–
- æ¨¡å‹æ¶æ§‹ï¼šWav2Vec2ForSpeechClassification (mean pooling + frozen CNN)
- è¨“ç·´æ¡†æ¶ï¼šHuggingFace Trainer (AdamW + linear LR scheduler)
- è³‡æ–™è™•ç†ï¼šå®Œæ•´èªéŸ³é•·åº¦ï¼ˆä¸æˆªæ–·ï¼‰+ Wav2Vec2Processor.pad
- è©•ä¼°ï¼švalidation set é¸æœ€ä½³æ¨¡å‹ + classification_report + confusion_matrix + ROC
- å¯é‡ç¾æ€§ï¼šè¨­å®š random seed
"""

import os
import argparse
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchaudio
import matplotlib.pyplot as plt

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, List, Union, Any
from math import sqrt

from datasets import Dataset as HFDataset
from transformers import (
    Wav2Vec2Processor,
    Wav2Vec2Config,
    Wav2Vec2PreTrainedModel,
    Wav2Vec2Model,
    AutoConfig,
    Trainer,
    TrainingArguments,
    EvalPrediction,
    set_seed,
)
from transformers.file_utils import ModelOutput
from packaging import version

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    mean_squared_error,
)

# ============================================================
#  è¨­å®šå€ â€” ä¾æ“šå¯¦é©—ç’°å¢ƒä¿®æ”¹
# ============================================================
TRAIN_CSV = "./experiment_sisman_scientific/scenario_A_screening/train.csv"
TEST_CSV = "./experiment_sisman_scientific/scenario_A_screening/test.csv"
AUDIO_ROOT = "/export/fs05/hyeh10/depression/daic_5utt_full/merged_5"

MODEL_NAME = "facebook/wav2vec2-base"
OUTPUT_DIR = "./output_scenario_A_v2"

# è¨“ç·´è¶…åƒæ•¸ â€” èˆ‡ 6+7+8 pipeline å°é½Š
SEED = 42
NUM_EPOCHS = 20
LEARNING_RATE = 5e-5
PER_DEVICE_TRAIN_BATCH_SIZE = 4
PER_DEVICE_EVAL_BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 2
FP16 = torch.cuda.is_available()
EVAL_STEPS = 50
SAVE_STEPS = 50
LOGGING_STEPS = 50
SAVE_TOTAL_LIMIT = 3
WARMUP_RATIO = 0.1

# Label å°ç…§è¡¨
LABEL_MAP = {"non": 0, "0": 0, 0: 0, "dep": 1, "1": 1, 1: 1}
LABEL_NAMES = ["non-depressed", "depressed"]

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ============================================================
#  æ¨¡å‹å®šç¾© â€” å®Œå…¨å°æ‡‰ build_model.py (File 6)
# ============================================================

@dataclass
class SpeechClassifierOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None


class Wav2Vec2ClassificationHead(nn.Module):
    """åˆ†é¡é ­ â€” èˆ‡ build_model.py å®Œå…¨ä¸€è‡´"""
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, features, **kwargs):
        x = features
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):
    """
    æ¨¡å‹é¡ â€” èˆ‡ build_model.py (File 6) å®Œå…¨ä¸€è‡´
    è¨­å®š pooling_mode="mean" æ™‚ç­‰åŒæ–¼ HuangForSpeechClassification
    """
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.pooling_mode = config.pooling_mode
        self.config = config

        self.wav2vec2 = Wav2Vec2Model(config)
        self.classifier = Wav2Vec2ClassificationHead(config)
        self.init_weights()

    def freeze_feature_extractor(self):
        self.wav2vec2.feature_extractor._freeze_parameters()

    def merged_strategy(self, hidden_states, mode="mean"):
        if mode == "mean":
            outputs = torch.mean(hidden_states, dim=1)
        elif mode == "max":
            outputs = torch.max(hidden_states, dim=1)[0]
        else:
            raise Exception("Pooling methods: 'mean', 'max'")
        return outputs

    def forward(
        self,
        input_values,
        attention_mask=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        labels=None,
    ):
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )
        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = outputs[0]
        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)
        logits = self.classifier(hidden_states)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels > 1 and (
                    labels.dtype == torch.long or labels.dtype == torch.int
                ):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "single_label_classification":
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = nn.BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SpeechClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


# ============================================================
#  DataCollator â€” å®Œå…¨å°æ‡‰ build_model.py (File 6)
# ============================================================

@dataclass
class DataCollatorCTCWithPadding:
    """
    Data collator â€” ä½¿ç”¨ Wav2Vec2Processor.pad é€²è¡Œå‹•æ…‹ padding
    èˆ‡ build_model.py å®Œå…¨ä¸€è‡´
    """
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_features = [
            {"input_values": feature["input_values"]} for feature in features
        ]
        label_features = [feature["labels"] for feature in features]

        d_type = (
            torch.long if isinstance(label_features[0], int) else torch.float
        )

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        batch["labels"] = torch.tensor(label_features, dtype=d_type)
        return batch


# ============================================================
#  compute_metrics â€” å®Œå…¨å°æ‡‰ build_model.py (File 6)
# ============================================================

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.argmax(preds, axis=1)
    acc = accuracy_score(p.label_ids, preds)
    f1 = f1_score(p.label_ids, preds, average="macro")
    return {"accuracy": acc, "f1": f1}


# ============================================================
#  CTCTrainer â€” å®Œå…¨å°æ‡‰ train.py (File 7)
# ============================================================

if version.parse(torch.__version__) >= version.parse("1.6"):
    _is_native_amp_available = True
    from torch.cuda.amp import autocast


class CTCTrainer(Trainer):
    """
    è‡ªè¨‚ Trainer â€” èˆ‡ train.py (File 7) çš„ CTCTrainer å®Œå…¨ä¸€è‡´
    æ”¯æ´ AMP æ··åˆç²¾åº¦è¨“ç·´
    """
    def training_step(
        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]
    ) -> torch.Tensor:
        model.train()
        inputs = self._prepare_inputs(inputs)

        if self.use_amp:
            with autocast():
                loss = self.compute_loss(model, inputs)
        else:
            loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps

        if self.use_amp:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()

        return loss.detach()


# ============================================================
#  è³‡æ–™è¼‰å…¥èˆ‡é è™•ç†
# ============================================================

def load_audio_dataset(csv_path: str) -> HFDataset:
    """å¾ CSV è¼‰å…¥è³‡æ–™é›†ï¼Œè½‰æ›ç‚º HuggingFace Dataset æ ¼å¼"""
    df = pd.read_csv(csv_path)
    print(f"ğŸ“‚ è®€å– {csv_path}ï¼Œå…± {len(df)} ç­†è³‡æ–™")

    records = []
    skipped = 0
    for _, row in df.iterrows():
        wav_path = os.path.join(AUDIO_ROOT, row["path"])
        raw_label = str(row["label"]).strip().lower()

        if raw_label not in LABEL_MAP:
            skipped += 1
            continue
        if not os.path.exists(wav_path):
            print(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {wav_path}")
            skipped += 1
            continue

        records.append({
            "path": wav_path,
            "label": LABEL_MAP[raw_label],
        })

    if skipped > 0:
        print(f"âš ï¸ è·³é {skipped} ç­†ç„¡æ•ˆ/ä¸å­˜åœ¨çš„è³‡æ–™")
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(records)} ç­†è³‡æ–™")

    return HFDataset.from_dict({
        "path": [r["path"] for r in records],
        "label": [r["label"] for r in records],
    })


def speech_file_to_array_fn(batch, processor):
    """
    å°‡éŸ³è¨Šæª”æ¡ˆè®€å–ä¸¦è½‰æ›ç‚º array â€” ä¸æˆªæ–·ï¼Œä½¿ç”¨å®Œæ•´èªéŸ³é•·åº¦
    """
    speech_array, sampling_rate = torchaudio.load(batch["path"])

    # å¤šè²é“è½‰å–®è²é“
    if speech_array.shape[0] > 1:
        speech_array = torch.mean(speech_array, dim=0, keepdim=True)
    speech_array = speech_array.squeeze().numpy()

    # é‡å–æ¨£è‡³ 16kHz
    if sampling_rate != 16000:
        import librosa
        speech_array = librosa.resample(
            speech_array, orig_sr=sampling_rate, target_sr=16000
        )

    batch["speech"] = speech_array
    return batch


def preprocess_function(batch, processor):
    """å°‡ speech array è½‰ç‚º input_values"""
    result = processor(
        batch["speech"],
        sampling_rate=16000,
        return_tensors="np",
        padding=False,
    )
    batch["input_values"] = result.input_values[0]
    batch["labels"] = batch["label"]
    return batch


def split_train_valid(dataset: HFDataset, valid_ratio: float = 0.15, seed: int = 42):
    """
    å¾è¨“ç·´é›†ä¸­åˆ†å‡ºé©—è­‰é›† â€” å°æ‡‰ 6+7+8 çš„ train/valid/test ä¸‰åˆ†çµæ§‹
    """
    split = dataset.train_test_split(test_size=valid_ratio, seed=seed)
    return split["train"], split["test"]


# ============================================================
#  è©•ä¼°èˆ‡å ±å‘Š â€” å°æ‡‰ evaluate.py (File 8)
# ============================================================

def full_evaluation(trainer, test_dataset, config_obj, output_dir):
    """
    å®Œæ•´è©•ä¼° â€” èˆ‡ evaluate.py (File 8) ä¸€è‡´
    åŒ…å«ï¼šclassification_report + confusion_matrix + ROC curve
    """
    predictions = trainer.predict(test_dataset)
    preds = predictions.predictions
    if isinstance(preds, tuple):
        preds = preds[0]
    y_pred = np.argmax(preds, axis=1)
    y_true = predictions.label_ids

    # Classification Report
    print("\n" + "=" * 60)
    print("ğŸ“Š Classification Report")
    print("=" * 60)
    report = classification_report(
        y_true, y_pred,
        target_names=LABEL_NAMES,
        zero_division=0,
        output_dict=True,
    )
    report_df = pd.DataFrame(report).transpose()
    print(report_df)

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    cm_df = pd.DataFrame(cm, index=LABEL_NAMES, columns=LABEL_NAMES)
    print("\nğŸ“Š Confusion Matrix:")
    print(cm_df)

    # MSE / RMSE
    mse = mean_squared_error(y_true, y_pred)
    rmse = sqrt(mse)
    report_df["MSE"] = mse
    report_df["RMSE"] = rmse

    # ROC Curve (binary)
    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc = auc(fpr, tpr)

    # å„²å­˜çµæœ
    results_path = os.path.join(output_dir, "results")
    os.makedirs(results_path, exist_ok=True)

    report_df.to_csv(os.path.join(results_path, "clsf_report.csv"), sep="\t")
    cm_df.to_csv(os.path.join(results_path, "conf_matrix.csv"), sep="\t")

    plt.figure()
    plt.plot(fpr, tpr, color="darkorange", lw=2,
             label=f"ROC curve (AUC = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve - Scenario A")
    plt.legend(loc="lower right")
    plt.savefig(os.path.join(results_path, "roc_curve.png"))
    plt.close()
    print(f"\nâœ… çµæœå·²å„²å­˜è‡³ {results_path}")

    # é¡å¤–è¼¸å‡º accuracy èˆ‡ F1
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="macro")
    print(f"\nğŸ¯ Test Accuracy: {acc:.4f}")
    print(f"ğŸ¯ Test F1 (macro): {f1:.4f}")
    print(f"ğŸ“ˆ AUC: {roc_auc:.4f}")

    return {"accuracy": acc, "f1": f1, "auc": roc_auc}


# ============================================================
#  ä¸»è¨“ç·´æµç¨‹
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Scenario A â€” ä½¿ç”¨ 6+7+8 ç­‰æ•ˆè¨“ç·´ç®¡ç·š")
    print("   æ¨¡å‹: Wav2Vec2ForSpeechClassification (mean pooling)")
    print("   CNN: Frozen (feature extractor)")
    print("   åˆ†é¡: Binary (depressed / non-depressed)")
    print("   éŸ³è¨Š: å®Œæ•´é•·åº¦ï¼ˆä¸æˆªæ–·ï¼‰")
    print("=" * 60)

    # 1. è¨­å®š seed â€” å°æ‡‰ train.py çš„ set_seed()
    set_seed(SEED)
    print(f"ğŸ² Random seed: {SEED}")

    # 2. è¼‰å…¥ processor èˆ‡ config
    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    config = Wav2Vec2Config.from_pretrained(
        MODEL_NAME,
        num_labels=2,
        final_dropout=0.1,
        pooling_mode="mean",  # æ˜ç¢ºè¨­å®š mean pooling
    )

    # 3. è¼‰å…¥æ¨¡å‹
    model = Wav2Vec2ForSpeechClassification.from_pretrained(
        MODEL_NAME, config=config
    )

    # 4. å‡çµ feature extractor (CNN) â€” å°æ‡‰ train.py çš„ freeze_feature_extractor
    model.freeze_feature_extractor()
    print("â„ï¸ Feature Extractor (CNN) å·²å‡çµ")
    print(f"ğŸ” Transformer ç¬¬ä¸€å±¤æ¢¯åº¦: "
          f"{model.wav2vec2.encoder.layers[0].attention.k_proj.weight.requires_grad}")

    # 5. è¼‰å…¥è³‡æ–™
    print("\nğŸ“¦ è¼‰å…¥è³‡æ–™é›†...")
    train_dataset_full = load_audio_dataset(TRAIN_CSV)
    test_dataset = load_audio_dataset(TEST_CSV)

    # 6. é è™•ç†éŸ³è¨Š â€” ä½¿ç”¨å®Œæ•´èªéŸ³é•·åº¦ï¼ˆä¸æˆªæ–·ï¼‰
    print("\nğŸ”Š é è™•ç†éŸ³è¨Šæª”æ¡ˆ...")
    train_dataset_full = train_dataset_full.map(
        speech_file_to_array_fn,
        fn_kwargs={"processor": processor},
    )
    test_dataset = test_dataset.map(
        speech_file_to_array_fn,
        fn_kwargs={"processor": processor},
    )

    # 7. è½‰æ›ç‚º input_values
    print("ğŸ”„ è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼...")
    train_dataset_full = train_dataset_full.map(
        preprocess_function,
        fn_kwargs={"processor": processor},
    )
    test_dataset = test_dataset.map(
        preprocess_function,
        fn_kwargs={"processor": processor},
    )

    # 8. åˆ†å‰² train/valid â€” å°æ‡‰ 6+7+8 çš„ä¸‰åˆ†çµæ§‹
    train_dataset, eval_dataset = split_train_valid(
        train_dataset_full, valid_ratio=0.15, seed=SEED
    )
    print(f"ğŸ“Š Train: {len(train_dataset)} ç­† | Valid: {len(eval_dataset)} ç­† | "
          f"Test: {len(test_dataset)} ç­†")

    # 9. è¨­å®š DataCollator â€” å°æ‡‰ build_model.py çš„ DataCollatorCTCWithPadding
    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)

    # 10. è¨­å®š TrainingArguments â€” å°æ‡‰ train.py (File 7)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        evaluation_strategy="steps",
        num_train_epochs=NUM_EPOCHS,
        fp16=FP16,
        save_steps=SAVE_STEPS,
        eval_steps=EVAL_STEPS,
        logging_steps=LOGGING_STEPS,
        learning_rate=LEARNING_RATE,
        save_total_limit=SAVE_TOTAL_LIMIT,
        seed=SEED,
        data_seed=SEED,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        greater_is_better=True,
        warmup_ratio=WARMUP_RATIO,
        report_to="none",
    )

    # 11. åˆå§‹åŒ– Trainer â€” ä½¿ç”¨ CTCTrainer (å°æ‡‰ train.py)
    trainer = CTCTrainer(
        model=model,
        data_collator=data_collator,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=processor.feature_extractor,
    )

    # 12. é–‹å§‹è¨“ç·´
    print("\nâš”ï¸ é–‹å§‹è¨“ç·´...")
    try:
        trainer.train()
    except RuntimeError as exception:
        if "out of memory" in str(exception):
            print("âš ï¸ GPU è¨˜æ†¶é«”ä¸è¶³ï¼å˜—è©¦æ¸…é™¤å¿«å–...")
            if hasattr(torch.cuda, "empty_cache"):
                torch.cuda.empty_cache()
        else:
            raise exception

    # 13. å„²å­˜æœ€ä½³æ¨¡å‹
    best_model_path = os.path.join(OUTPUT_DIR, "best_model")
    trainer.save_model(best_model_path)
    processor.save_pretrained(best_model_path)
    print(f"\nğŸ’¾ æœ€ä½³æ¨¡å‹å·²å„²å­˜è‡³: {best_model_path}")

    # 14. å®Œæ•´è©•ä¼° â€” å°æ‡‰ evaluate.py (File 8)
    print("\nğŸ“Š åœ¨æ¸¬è©¦é›†ä¸Šé€²è¡Œå®Œæ•´è©•ä¼°...")
    results = full_evaluation(trainer, test_dataset, config, OUTPUT_DIR)

    print("\nğŸ Scenario A å¯¦é©—å®Œæˆï¼")
