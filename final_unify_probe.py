"""
Speaker Probe â€” å…­çµ„æ¨¡å‹ Speaker Accuracy çµ±ä¸€è©•ä¼°
====================================================
é‚è¼¯ï¼šå°æ¯çµ„æ¨¡å‹æŠ½å‡º embeddingï¼Œç”¨ Logistic Regression é æ¸¬ Speaker IDã€‚
      Speaker Accuracy è¶Šä½ â†’ æ¨¡å‹è¶ŠæˆåŠŸå»é™¤ speaker è³‡è¨Šã€‚

å…­çµ„è¨­å®šï¼š
  1. Huang A   â†’ å¾®èª¿å¾Œ Wav2Vec2ï¼Œ768 ç¶­ mean pooling
  2. Huang B   â†’ å¾®èª¿å¾Œ Wav2Vec2ï¼Œ768 ç¶­ mean pooling
  3. Linear A  â†’ åŸå§‹å‡çµ Wav2Vec2ï¼Œ768 ç¶­ï¼ˆä¸éœ€è¦æ¨¡å‹æª”ï¼‰
  4. Linear B  â†’ åŸå§‹å‡çµ Wav2Vec2ï¼Œ768 ç¶­ï¼ˆä¸éœ€è¦æ¨¡å‹æª”ï¼‰
  5. DANN A    â†’ å‡çµ Wav2Vec2 â†’ shared_encoderï¼Œ128 ç¶­
  6. DANN B    â†’ å‡çµ Wav2Vec2 â†’ shared_encoderï¼Œ128 ç¶­

æ¯çµ„è·‘ 5 æ¬¡ï¼ˆå°æ‡‰ 5 å€‹ runï¼‰ï¼Œæœ€å¾Œè¼¸å‡ºå¹³å‡ Â± æ¨™æº–å·®ã€‚
è‹¥æŸå€‹ run çš„æ¨¡å‹æª”ä¸å­˜åœ¨å‰‡è‡ªå‹•è·³éã€‚

ã€è·¯å¾‘èªªæ˜ã€‘
  åŸ·è¡Œå‰è«‹å°‡ä¸‹æ–¹ ======= è·¯å¾‘è¨­å®šå€ ======= å…§çš„è·¯å¾‘æ”¹æˆä½ çš„å¯¦éš›è·¯å¾‘ã€‚
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchaudio
from tqdm import tqdm
from transformers import Wav2Vec2Processor, Wav2Vec2Model, AutoConfig
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


# ============================================================
#  ======= è·¯å¾‘è¨­å®šå€ï¼ˆåŸ·è¡Œå‰è«‹ä¿®æ”¹ï¼‰ =======
# ============================================================

AUDIO_ROOT = ""   # CSV å…§å·²æ˜¯çµ•å°è·¯å¾‘ï¼Œä¿æŒç©ºå­—ä¸²å³å¯

# è³‡æ–™ CSV
CSV_A_TRAIN = "./experiment_sisman_scientific/scenario_A_screening/train.csv"
CSV_A_TEST  = "./experiment_sisman_scientific/scenario_A_screening/test.csv"
CSV_B_TRAIN = "./experiment_sisman_scientific/scenario_B_monitoring/train.csv"
CSV_B_TEST  = "./experiment_sisman_scientific/scenario_B_monitoring/test.csv"

# Huang best_model è·¯å¾‘æ¨£æ¿ï¼ˆ{run_i} æœƒè¢«æ›¿æ›æˆ 1~5ï¼‰
HUANG_A_MODEL_TEMPLATE = "./output_scenario_A_v2/run_{run_i}/best_model"
HUANG_B_MODEL_TEMPLATE = "./output_scenario_B_v2/run_{run_i}/best_model"

# DANN shared_encoder è·¯å¾‘æ¨£æ¿ï¼ˆ{run_i} æœƒè¢«æ›¿æ›æˆ 1~5ï¼‰
DANN_A_ENCODER_TEMPLATE = "./dann_A_shared_encoder_run_{run_i}.pth"
DANN_B_ENCODER_TEMPLATE = "./dann_B_shared_encoder_run_{run_i}.pth"

MODEL_NAME = "facebook/wav2vec2-base"
TOTAL_RUNS = 5

# ============================================================

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {DEVICE}")


# ============================================================
#  æ¨¡å‹çµæ§‹å®šç¾©
# ============================================================

class SharedEncoder(nn.Module):
    """å°æ‡‰ DANN çš„ shared_encoderï¼Œè¼¸å‡º 128 ç¶­"""
    def __init__(self, input_dim=768, hidden_dim=128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
    def forward(self, x):
        return self.encoder(x)


class Wav2Vec2ClassificationHead(nn.Module):
    """å°æ‡‰ build_model.py çš„ headï¼Œçµæ§‹éœ€å®Œæ•´æ‰èƒ½æ­£ç¢ºè¼‰å…¥ checkpoint"""
    def __init__(self, config):
        super().__init__()
        self.dense    = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout  = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, x):
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class Wav2Vec2ForSpeechClassification(nn.Module):
    """å°æ‡‰ build_model.py çš„å®Œæ•´æ¨¡å‹ï¼Œprobe åªç”¨ get_embedding()"""
    def __init__(self, config):
        super().__init__()
        self.wav2vec2   = Wav2Vec2Model(config)
        self.classifier = Wav2Vec2ClassificationHead(config)

    def get_embedding(self, input_values):
        """å›å‚³ mean pooling å¾Œçš„ 768 ç¶­ç‰¹å¾µ"""
        hidden_states = self.wav2vec2(input_values).last_hidden_state
        return torch.mean(hidden_states, dim=1)


# ============================================================
#  å·¥å…·å‡½å¼
# ============================================================

def extract_speaker_id(filepath):
    """å¾è·¯å¾‘å–å‡º speaker IDï¼ˆæª”ååº•ç·šå‰çš„éƒ¨åˆ†ï¼Œä¾‹å¦‚ 300_xxx.wav â†’ 300ï¼‰"""
    return os.path.basename(str(filepath)).split('_')[0]


def load_raw_w2v_embeddings(csv_path, processor, w2v_model):
    """
    [Linear Probing ç”¨]
    ç›´æ¥ç”¨å®Œå…¨å‡çµçš„ Wav2Vec2 æŠ½ 768 ç¶­ç‰¹å¾µã€‚
    ä»»ä½•æ™‚å€™é‡æŠ½çµæœéƒ½ç›¸åŒï¼Œä¸éœ€è¦æ¨¡å‹æª”ã€‚
    å›å‚³ï¼šX (N, 768), speaker_ids (N,)
    """
    df = pd.read_csv(csv_path)
    feats, spks = [], []

    w2v_model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), leave=False,
                           desc=f"  Raw W2V â† {os.path.basename(csv_path)}"):
            wav_path = os.path.join(AUDIO_ROOT, str(row['path']))
            try:
                waveform, sr = torchaudio.load(wav_path)
                if sr != 16000:
                    waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)
                inputs = processor(waveform.squeeze().numpy(),
                                   sampling_rate=16000,
                                   return_tensors="pt", padding=True)
                emb = w2v_model(**{k: v.to(DEVICE) for k, v in inputs.items()}
                                ).last_hidden_state.mean(dim=1).cpu().numpy()
                feats.append(emb.squeeze())
                spks.append(extract_speaker_id(row['path']))
            except:
                continue

    return np.array(feats), np.array(spks)


def load_huang_embeddings(csv_path, processor, model_dir):
    """
    [Huang ç”¨]
    è¼‰å…¥å¾®èª¿å¾Œçš„ best_modelï¼ŒæŠ½ 768 ç¶­ mean pooling ç‰¹å¾µã€‚
    å›å‚³ï¼šX (N, 768), speaker_ids (N,)
    """
    config = AutoConfig.from_pretrained(model_dir)
    model  = Wav2Vec2ForSpeechClassification(config).to(DEVICE)
    state_dict_path = os.path.join(model_dir, "pytorch_model.bin")
    model.load_state_dict(
        torch.load(state_dict_path, map_location=DEVICE), strict=False
    )
    model.eval()

    df = pd.read_csv(csv_path)
    feats, spks = [], []

    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), leave=False,
                           desc=f"  Huang â† {os.path.basename(csv_path)}"):
            wav_path = os.path.join(AUDIO_ROOT, str(row['path']))
            try:
                waveform, sr = torchaudio.load(wav_path)
                if sr != 16000:
                    waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)
                inputs = processor(waveform.squeeze().numpy(),
                                   sampling_rate=16000,
                                   return_tensors="pt", padding=True)
                emb = model.get_embedding(
                    inputs['input_values'].to(DEVICE)
                ).cpu().numpy()
                feats.append(emb.squeeze())
                spks.append(extract_speaker_id(row['path']))
            except:
                continue

    return np.array(feats), np.array(spks)


def load_dann_embeddings(csv_path, processor, w2v_model, encoder_path):
    """
    [DANN ç”¨]
    å‡çµ Wav2Vec2 å…ˆæŠ½ 768 ç¶­ï¼Œå†é shared_encoder å£“æˆ 128 ç¶­ã€‚
    å›å‚³ï¼šX (N, 128), speaker_ids (N,)
    """
    shared_encoder = SharedEncoder().to(DEVICE)
    shared_encoder.load_state_dict(
        torch.load(encoder_path, map_location=DEVICE)
    )
    shared_encoder.eval()

    df = pd.read_csv(csv_path)
    feats, spks = [], []

    w2v_model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), leave=False,
                           desc=f"  DANN â† {os.path.basename(csv_path)}"):
            wav_path = os.path.join(AUDIO_ROOT, str(row['path']))
            try:
                waveform, sr = torchaudio.load(wav_path)
                if sr != 16000:
                    waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)
                inputs = processor(waveform.squeeze().numpy(),
                                   sampling_rate=16000,
                                   return_tensors="pt", padding=True)
                raw_emb = w2v_model(**{k: v.to(DEVICE) for k, v in inputs.items()}
                                    ).last_hidden_state.mean(dim=1)
                emb = shared_encoder(raw_emb).cpu().numpy()
                feats.append(emb.squeeze())
                spks.append(extract_speaker_id(row['path']))
            except:
                continue

    return np.array(feats), np.array(spks)


def run_speaker_probe(X_train, spk_train, X_test, spk_test):
    """
    åœ¨ X_train ä¸Šè¨“ç·´ Logistic Regression é æ¸¬ speakerï¼Œ
    åœ¨ X_test ä¸Šè©•ä¼°ï¼Œå›å‚³ speaker accuracyã€‚
    """
    clf = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)
    clf.fit(X_train, spk_train)
    return accuracy_score(spk_test, clf.predict(X_test))


# ============================================================
#  ä¸»ç¨‹å¼
# ============================================================

if __name__ == "__main__":

    print("\nğŸ§  è¼‰å…¥å‡çµ Wav2Vec2ï¼ˆLinear Probing å’Œ DANN å…±ç”¨ï¼‰...")
    processor  = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    w2v_frozen = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)
    w2v_frozen.eval()

    # ----------------------------------------------------------
    # Linear Probing ç‰¹å¾µåªéœ€æŠ½ä¸€æ¬¡ï¼ˆå‡çµæ¨¡å‹ï¼Œæ¯æ¬¡çµæœç›¸åŒï¼‰
    # ----------------------------------------------------------
    print("\nğŸ“¦ æŠ½å– Linear Probing åŸå§‹ç‰¹å¾µï¼ˆåªéœ€ä¸€æ¬¡ï¼‰...")
    lp_A_train_X, lp_A_train_spk = load_raw_w2v_embeddings(CSV_A_TRAIN, processor, w2v_frozen)
    lp_A_test_X,  lp_A_test_spk  = load_raw_w2v_embeddings(CSV_A_TEST,  processor, w2v_frozen)
    lp_B_train_X, lp_B_train_spk = load_raw_w2v_embeddings(CSV_B_TRAIN, processor, w2v_frozen)
    lp_B_test_X,  lp_B_test_spk  = load_raw_w2v_embeddings(CSV_B_TEST,  processor, w2v_frozen)

    # ----------------------------------------------------------
    # å…­çµ„çµæœæ”¶é›†å®¹å™¨
    # ----------------------------------------------------------
    results = {
        "Huang_A":  [], "Huang_B":  [],
        "Linear_A": [], "Linear_B": [],
        "DANN_A":   [], "DANN_B":   [],
    }

    # ----------------------------------------------------------
    # é€ run è©•ä¼°
    # ----------------------------------------------------------
    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Run {run_i} / {TOTAL_RUNS}")
        print(f"{'='*60}")

        # â”€â”€ 1. Huang A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        huang_A_path = HUANG_A_MODEL_TEMPLATE.format(run_i=run_i)
        if os.path.exists(huang_A_path):
            print(f"\n[Huang A] è¼‰å…¥: {huang_A_path}")
            X_tr, spk_tr = load_huang_embeddings(CSV_A_TRAIN, processor, huang_A_path)
            X_te, spk_te = load_huang_embeddings(CSV_A_TEST,  processor, huang_A_path)
            acc = run_speaker_probe(X_tr, spk_tr, X_te, spk_te)
            results["Huang_A"].append(acc)
            print(f"  â†’ Speaker Acc: {acc:.4f}")
        else:
            print(f"  âš ï¸  Huang A Run {run_i} ä¸å­˜åœ¨ï¼Œè·³é ({huang_A_path})")

        # â”€â”€ 2. Huang B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        huang_B_path = HUANG_B_MODEL_TEMPLATE.format(run_i=run_i)
        if os.path.exists(huang_B_path):
            print(f"\n[Huang B] è¼‰å…¥: {huang_B_path}")
            X_tr, spk_tr = load_huang_embeddings(CSV_B_TRAIN, processor, huang_B_path)
            X_te, spk_te = load_huang_embeddings(CSV_B_TEST,  processor, huang_B_path)
            acc = run_speaker_probe(X_tr, spk_tr, X_te, spk_te)
            results["Huang_B"].append(acc)
            print(f"  â†’ Speaker Acc: {acc:.4f}")
        else:
            print(f"  âš ï¸  Huang B Run {run_i} ä¸å­˜åœ¨ï¼Œè·³é ({huang_B_path})")

        # â”€â”€ 3. Linear Probing Aï¼ˆç‰¹å¾µå·²æŠ½å¥½ï¼Œç›´æ¥ probeï¼‰â”€â”€
        print(f"\n[Linear A] å‡çµ Wav2Vec2 ç‰¹å¾µï¼Œç›´æ¥ probe")
        acc = run_speaker_probe(lp_A_train_X, lp_A_train_spk,
                                lp_A_test_X,  lp_A_test_spk)
        results["Linear_A"].append(acc)
        print(f"  â†’ Speaker Acc: {acc:.4f}")

        # â”€â”€ 4. Linear Probing Bï¼ˆç‰¹å¾µå·²æŠ½å¥½ï¼Œç›´æ¥ probeï¼‰â”€â”€
        print(f"\n[Linear B] å‡çµ Wav2Vec2 ç‰¹å¾µï¼Œç›´æ¥ probe")
        acc = run_speaker_probe(lp_B_train_X, lp_B_train_spk,
                                lp_B_test_X,  lp_B_test_spk)
        results["Linear_B"].append(acc)
        print(f"  â†’ Speaker Acc: {acc:.4f}")

        # â”€â”€ 5. DANN A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        dann_A_path = DANN_A_ENCODER_TEMPLATE.format(run_i=run_i)
        if os.path.exists(dann_A_path):
            print(f"\n[DANN A] è¼‰å…¥: {dann_A_path}")
            X_tr, spk_tr = load_dann_embeddings(CSV_A_TRAIN, processor, w2v_frozen, dann_A_path)
            X_te, spk_te = load_dann_embeddings(CSV_A_TEST,  processor, w2v_frozen, dann_A_path)
            acc = run_speaker_probe(X_tr, spk_tr, X_te, spk_te)
            results["DANN_A"].append(acc)
            print(f"  â†’ Speaker Acc: {acc:.4f}")
        else:
            print(f"  âš ï¸  DANN A Run {run_i} ä¸å­˜åœ¨ï¼Œè·³é ({dann_A_path})")

        # â”€â”€ 6. DANN B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        dann_B_path = DANN_B_ENCODER_TEMPLATE.format(run_i=run_i)
        if os.path.exists(dann_B_path):
            print(f"\n[DANN B] è¼‰å…¥: {dann_B_path}")
            X_tr, spk_tr = load_dann_embeddings(CSV_B_TRAIN, processor, w2v_frozen, dann_B_path)
            X_te, spk_te = load_dann_embeddings(CSV_B_TEST,  processor, w2v_frozen, dann_B_path)
            acc = run_speaker_probe(X_tr, spk_tr, X_te, spk_te)
            results["DANN_B"].append(acc)
            print(f"  â†’ Speaker Acc: {acc:.4f}")
        else:
            print(f"  âš ï¸  DANN B Run {run_i} ä¸å­˜åœ¨ï¼Œè·³é ({dann_B_path})")

    # ----------------------------------------------------------
    # å½™ç¸½è¼¸å‡º
    # ----------------------------------------------------------
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Speaker Probe å½™ç¸½çµæœ")
    print(f"{'='*60}")
    print(f"{'æ¨¡å‹':<12} {'æœ‰æ•ˆ runs':<10} {'å¹³å‡ Spk Acc':<16} æ¨™æº–å·®")
    print(f"{'â”€'*55}")

    summary_rows = []
    for name, accs in results.items():
        if len(accs) == 0:
            print(f"{name:<12} {'0':<10} {'N/A':<16} N/A")
        else:
            arr  = np.array(accs)
            mean = arr.mean()
            std  = arr.std()
            print(f"{name:<12} {len(accs):<10} {mean:.4f}{'':>10} Â± {std:.4f}")
            summary_rows.append({
                "model": name, "valid_runs": len(accs),
                "spk_acc_mean": round(mean, 4),
                "spk_acc_std":  round(std,  4),
            })

    if summary_rows:
        out_path = "speaker_probe_summary.csv"
        pd.DataFrame(summary_rows).to_csv(out_path, index=False)
        print(f"\nâœ… å½™ç¸½å·²å„²å­˜è‡³ {out_path}")
