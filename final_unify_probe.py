"""
Speaker Probe â€” å…«çµ„æ¨¡å‹ Speaker Accuracy çµ±ä¸€è©•ä¼°
====================================================
ã€è¨­è¨ˆé‚è¼¯ã€‘
  Speaker Probe çš„ç›®çš„æ˜¯è¡¡é‡ã€Œæ¨¡å‹çš„ latent representation è£¡
  æ®˜ç•™äº†å¤šå°‘ speaker identity è³‡è¨Šã€ï¼Œå› æ­¤ probe çš„å°è±¡
  æ‡‰è©²åš´æ ¼å°æ‡‰è«–æ–‡çš„å…©ç¨®æƒ…å¢ƒï¼š

  â”€â”€ Scenario Aï¼ˆStrict Screeningï¼Œunseen speakersï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Test set = 38 ä½ target speakers çš„ Current Dataï¼ˆ714 segsï¼‰
  Train set = 151 ä½ control speakersï¼ˆBase + Fillerï¼‰
  æ¨¡å‹å®Œå…¨æœªè¦‹é target speakersã€‚
  â†’ Probe classifier åœ¨ trainï¼ˆ151 äººï¼‰çš„ embedding ä¸Šè¨“ç·´ï¼Œ
    åœ¨ testï¼ˆ38 ä½ unseenï¼‰ä¸Šè©•ä¼°ã€‚
  â†’ Spk Acc ç†è«–ä¸Š â‰ˆ 0%ï¼Œèªç¾©ç‚ºã€Œprobe ç„¡æ³•è¾¨è­˜ unseen speakersã€ã€‚
    æ³¨æ„ï¼šé€™ä¸ä»£è¡¨æ¨¡å‹æ²’æœ‰ encode speaker infoï¼Œ
    è€Œæ˜¯ probe classifier æ ¹æœ¬æ²’è¦‹éé€™äº› classã€‚

  â”€â”€ Scenario Bï¼ˆLongitudinal Monitoringï¼Œseen speakersï¼‰â”€â”€â”€â”€â”€
  Train set = Baseï¼ˆ151 äººï¼‰+ Historicalï¼ˆ38 ä½ target çš„æ­·å²éŒ„éŸ³ï¼Œ714 segsï¼‰
  Test set  = åŒ 38 ä½ target çš„ Current Dataï¼ˆ714 segsï¼‰
  â†’ æ­£ç¢º probe æ‡‰åªç”¨ Historical é‚£ 714 ç­†ï¼ˆtarget 38 äººï¼‰
    ä¾†è¨“ç·´ probe classifierï¼Œç„¶å¾Œåœ¨ testï¼ˆåŒ 38 äººï¼‰è©•ä¼°ã€‚
  â†’ è‹¥ç”¨å…¨ 5117 ç­† trainï¼Œå®¹é‡è¢« 151 ä½ control ç¨€é‡‹ï¼ŒSpk Acc è¢«ä½ä¼°ã€‚
  â†’ ç¨‹å¼ä¸Šï¼šå¾ train_B CSV ä¸­ç¯©å‡ºå±¬æ–¼ target speakers çš„æ¨£æœ¬ã€‚

å…«çµ„è¨­å®šï¼š
  1. Huang A      â†’ å¾®èª¿å¾Œ Wav2Vec2ï¼Œ768 ç¶­ mean pooling
  2. Huang B      â†’ å¾®èª¿å¾Œ Wav2Vec2ï¼Œ768 ç¶­ mean pooling
  3. Linear A     â†’ åŸå§‹å‡çµ Wav2Vec2ï¼Œ768 ç¶­ï¼ˆä¸éœ€è¦æ¨¡å‹æª”ï¼‰
  4. Linear B     â†’ åŸå§‹å‡çµ Wav2Vec2ï¼Œ768 ç¶­ï¼ˆä¸éœ€è¦æ¨¡å‹æª”ï¼‰
  5. DANN A       â†’ å‡çµ Wav2Vec2 â†’ shared_encoderï¼Œ128 ç¶­ï¼ˆå¤–éƒ¨ .pthï¼‰
  6. DANN B       â†’ å‡çµ Wav2Vec2 â†’ shared_encoderï¼Œ128 ç¶­ï¼ˆå¤–éƒ¨ .pthï¼‰
  7. DANN-FT A    â†’ å®Œæ•´å¾®èª¿ Wav2Vec2DANNFinetune (Scenario A)ï¼Œ128 ç¶­
  8. DANN-FT B    â†’ å®Œæ•´å¾®èª¿ Wav2Vec2DANNFinetune (Scenario B)ï¼Œ128 ç¶­

æ¯çµ„è·‘ 5 æ¬¡ï¼ˆå°æ‡‰ 5 å€‹ runï¼‰ï¼Œæœ€å¾Œè¼¸å‡ºå¹³å‡ Â± æ¨™æº–å·®ã€‚
è‹¥æŸå€‹ run çš„æ¨¡å‹æª”ä¸å­˜åœ¨å‰‡è‡ªå‹•è·³éï¼ˆä¾‹å¦‚ DANN-FT é‚„åœ¨è¨“ç·´ä¸­ï¼‰ã€‚
æ”¯æ´ pytorch_model.bin å’Œ model.safetensors å…©ç¨®æ¬Šé‡æ ¼å¼ã€‚
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torchaudio
from dataclasses import dataclass
from typing import Optional, Set
from tqdm import tqdm
from torch.autograd import Function
from transformers import (
    Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2Config,
    Wav2Vec2PreTrainedModel, AutoConfig
)
from transformers.file_utils import ModelOutput
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


# ============================================================
#  ======= è·¯å¾‘è¨­å®šå€ï¼ˆåŸ·è¡Œå‰è«‹ä¿®æ”¹ï¼‰ =======
# ============================================================

AUDIO_ROOT = ""   # CSV å…§å·²æ˜¯çµ•å°è·¯å¾‘ï¼Œä¿æŒç©ºå­—ä¸²å³å¯

CSV_A_TRAIN = "./experiment_sisman_scientific/scenario_A_screening/train.csv"
CSV_A_TEST  = "./experiment_sisman_scientific/scenario_A_screening/test.csv"
CSV_B_TRAIN = "./experiment_sisman_scientific/scenario_B_monitoring/train.csv"
CSV_B_TEST  = "./experiment_sisman_scientific/scenario_B_monitoring/test.csv"

HUANG_A_MODEL_TEMPLATE  = "./output_scenario_A_v2/run_{run_i}/best_model"
HUANG_B_MODEL_TEMPLATE  = "./output_scenario_B_v2/run_{run_i}/best_model"

DANN_A_ENCODER_TEMPLATE = "./dann_A_shared_encoder_run_{run_i}.pth"
DANN_B_ENCODER_TEMPLATE = "./dann_B_shared_encoder_run_{run_i}.pth"

DANN_FT_A_DIR_TEMPLATE  = "./output_dann_finetune_A_v6/run_{run_i}"
DANN_FT_B_DIR_TEMPLATE  = "./output_dann_finetune_B_v6/run_{run_i}"

MODEL_NAME = "facebook/wav2vec2-base"
TOTAL_RUNS = 5

# ============================================================

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {DEVICE}")


# ============================================================
#  æ¨¡å‹çµæ§‹å®šç¾©
# ============================================================

class SharedEncoder(nn.Module):
    """å°æ‡‰èˆŠç‰ˆ DANN çš„ shared_encoderï¼ˆå¤–éƒ¨ .pthï¼‰ï¼Œè¼¸å‡º 128 ç¶­"""
    def __init__(self, input_dim=768, hidden_dim=128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
    def forward(self, x):
        return self.encoder(x)


class Wav2Vec2ClassificationHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense    = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout  = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, x):
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class Wav2Vec2ForSpeechClassification(nn.Module):
    """Huang baseline å®Œæ•´æ¨¡å‹ï¼Œprobe åªç”¨ get_embedding()"""
    def __init__(self, config):
        super().__init__()
        self.wav2vec2   = Wav2Vec2Model(config)
        self.classifier = Wav2Vec2ClassificationHead(config)

    def get_embedding(self, input_values):
        hidden_states = self.wav2vec2(input_values).last_hidden_state
        return torch.mean(hidden_states, dim=1)


# â”€â”€ DANN-FT ç›¸é—œçµæ§‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha): ctx.alpha = alpha; return x.view_as(x)
    @staticmethod
    def backward(ctx, grad_output): return grad_output.neg() * ctx.alpha, None

class GradientReversalLayer(nn.Module):
    def forward(self, x, alpha=1.0): return GradientReversalFn.apply(x, alpha)

@dataclass
class DANNOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    spk_logits: Optional[torch.FloatTensor] = None

class Wav2Vec2DANNFinetune(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.wav2vec2 = Wav2Vec2Model(config)
        self.shared_encoder = nn.Sequential(
            nn.Linear(config.hidden_size, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        self.dep_classifier = nn.Sequential(
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, config.num_labels)
        )
        self.spk_classifier = nn.Sequential(
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, getattr(config, "num_speakers", 151))
        )
        self.grl = GradientReversalLayer()
        self.init_weights()

    def get_embedding(self, input_values, attention_mask=None):
        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)
        pooled  = torch.mean(outputs[0], dim=1)
        return self.shared_encoder(pooled)

    def forward(self, input_values, attention_mask=None, return_dict=None,
                labels=None, speaker_labels=None, alpha=1.0):
        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)
        shared  = self.shared_encoder(torch.mean(outputs[0], dim=1))
        dep_logits = self.dep_classifier(shared)
        spk_logits = self.spk_classifier(self.grl(shared, alpha))
        loss = None
        if labels is not None:
            loss = nn.CrossEntropyLoss()(dep_logits.view(-1, self.config.num_labels), labels.view(-1))
        if speaker_labels is not None:
            mask = speaker_labels >= 0
            if mask.sum() > 0:
                loss_spk = nn.CrossEntropyLoss()(
                    spk_logits[mask].view(-1, spk_logits.size(-1)),
                    speaker_labels[mask].view(-1)
                )
                loss = loss + loss_spk if loss is not None else loss_spk
        use_return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        if not use_return_dict:
            return ((loss, dep_logits, spk_logits) if loss is not None else (dep_logits, spk_logits))
        return DANNOutput(loss=loss, logits=dep_logits, spk_logits=spk_logits)


# ============================================================
#  å·¥å…·å‡½å¼
# ============================================================

def extract_speaker_id(filepath: str) -> str:
    return os.path.basename(str(filepath)).split('_')[0]


def get_target_speaker_set(test_csv: str) -> Set[str]:
    """å¾ test_B CSV å–å‡º 38 ä½ target speaker IDã€‚"""
    df = pd.read_csv(test_csv)
    return set(df['path'].apply(extract_speaker_id).unique())


def _find_best_checkpoint(run_dir: str) -> Optional[str]:
    """æ‰¾æœ€æ–°çš„ checkpoint-XXXXï¼Œæ‰¾ä¸åˆ°å‰‡ fallback åˆ° run_dir æœ¬èº«ã€‚"""
    if not os.path.isdir(run_dir):
        return None
    checkpoints = sorted(
        [d for d in os.listdir(run_dir) if d.startswith("checkpoint-")],
        key=lambda x: int(x.split("-")[-1])
    )
    if checkpoints:
        return os.path.join(run_dir, checkpoints[-1])
    if os.path.exists(os.path.join(run_dir, "config.json")):
        return run_dir
    return None


def _has_model_weights(dirpath: str) -> bool:
    """æ”¯æ´ pytorch_model.bin å’Œ model.safetensorsã€‚"""
    return (
        os.path.exists(os.path.join(dirpath, "pytorch_model.bin")) or
        os.path.exists(os.path.join(dirpath, "model.safetensors"))
    )


def _load_waveform(wav_path: str, processor):
    """è¼‰å…¥éŸ³æª”ä¸¦å‰è™•ç†ï¼Œå¤±æ•—å›å‚³ Noneã€‚"""
    try:
        waveform, sr = torchaudio.load(wav_path)
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        if sr != 16000:
            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
        inputs = processor(waveform.squeeze().numpy(),
                           sampling_rate=16000,
                           return_tensors="pt", padding=True)
        return inputs
    except Exception:
        return None


# â”€â”€ Embedding æŠ½å–å‡½å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_raw_w2v_embeddings(csv_path, processor, w2v_model,
                             filter_speakers: Optional[Set[str]] = None):
    """å‡çµ Wav2Vec2ï¼ŒæŠ½ 768 ç¶­ mean poolingã€‚"""
    df = pd.read_csv(csv_path)
    feats, spks = [], []
    w2v_model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), leave=False,
                           desc=f"  Raw W2V â† {os.path.basename(csv_path)}"):
            spk_id = extract_speaker_id(row['path'])
            if filter_speakers is not None and spk_id not in filter_speakers:
                continue
            inputs = _load_waveform(os.path.join(AUDIO_ROOT, str(row['path'])), processor)
            if inputs is None:
                continue
            emb = w2v_model(**{k: v.to(DEVICE) for k, v in inputs.items()}
                            ).last_hidden_state.mean(dim=1).cpu().numpy()
            feats.append(emb.squeeze())
            spks.append(spk_id)
    return np.array(feats), np.array(spks)


def load_huang_embeddings(csv_path, processor, model_dir,
                          filter_speakers: Optional[Set[str]] = None):
    """
    Huang Baselineï¼šè¼‰å…¥ best_modelï¼ŒæŠ½ 768 ç¶­ã€‚
    æ”¯æ´ pytorch_model.bin å’Œ model.safetensorsã€‚
    """
    config = AutoConfig.from_pretrained(model_dir)
    model  = Wav2Vec2ForSpeechClassification(config).to(DEVICE)

    bin_path  = os.path.join(model_dir, "pytorch_model.bin")
    safe_path = os.path.join(model_dir, "model.safetensors")

    if os.path.exists(bin_path):
        model.load_state_dict(torch.load(bin_path, map_location=DEVICE), strict=False)
    elif os.path.exists(safe_path):
        from safetensors.torch import load_file
        model.load_state_dict(load_file(safe_path, device=DEVICE), strict=False)
    else:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ¬Šé‡æª”ï¼š{model_dir}")

    model.eval()
    df = pd.read_csv(csv_path)
    feats, spks = [], []
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), leave=False,
                           desc=f"  Huang â† {os.path.basename(csv_path)}"):
            spk_id = extract_speaker_id(row['path'])
            if filter_speakers is not None and spk_id not in filter_speakers:
                continue
            inputs = _load_waveform(os.path.join(AUDIO_ROOT, str(row['path'])), processor)
            if inputs is None:
                continue
            emb = model.get_embedding(inputs['input_values'].to(DEVICE)).cpu().numpy()
            feats.append(emb.squeeze())
            spks.append(spk_id)
    return np.array(feats), np.array(spks)


def load_dann_embeddings(csv_path, processor, w2v_model, encoder_path,
                         filter_speakers: Optional[Set[str]] = None):
    """
    èˆŠç‰ˆ DANNï¼šå‡çµ W2Vï¼ˆ768 ç¶­ï¼‰â†’ å¤–éƒ¨ shared_encoderï¼ˆ128 ç¶­ï¼‰ã€‚

    èˆŠç‰ˆå„²å­˜çš„ .pth key æ ¼å¼ç‚º "0.weight", "1.weight" ...
    ä½† SharedEncoder åŒ…äº†ä¸€å±¤ self.encoderï¼Œkey ç‚º "encoder.0.weight" ...
    è¼‰å…¥å‰è‡ªå‹•åš key æ˜ å°„ï¼Œå…©ç¨®æ ¼å¼éƒ½ç›¸å®¹ã€‚
    """
    shared_encoder = SharedEncoder().to(DEVICE)
    raw_sd = torch.load(encoder_path, map_location=DEVICE)
    # è‹¥ key æ²’æœ‰ "encoder." å‰ç¶´ï¼Œè‡ªå‹•è£œä¸Š
    if any(not k.startswith("encoder.") for k in raw_sd.keys()):
        raw_sd = {f"encoder.{k}": v for k, v in raw_sd.items()}
    shared_encoder.load_state_dict(raw_sd)
    shared_encoder.eval()
    df = pd.read_csv(csv_path)
    feats, spks = [], []
    w2v_model.eval()
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), leave=False,
                           desc=f"  DANN â† {os.path.basename(csv_path)}"):
            spk_id = extract_speaker_id(row['path'])
            if filter_speakers is not None and spk_id not in filter_speakers:
                continue
            inputs = _load_waveform(os.path.join(AUDIO_ROOT, str(row['path'])), processor)
            if inputs is None:
                continue
            raw_emb = w2v_model(**{k: v.to(DEVICE) for k, v in inputs.items()}
                                ).last_hidden_state.mean(dim=1)
            emb = shared_encoder(raw_emb).cpu().numpy()
            feats.append(emb.squeeze())
            spks.append(spk_id)
    return np.array(feats), np.array(spks)


def load_dann_ft_embeddings(csv_path, processor, checkpoint_dir,
                            num_speakers=None,
                            filter_speakers: Optional[Set[str]] = None):
    """
    DANN-FTï¼šå¾ Trainer checkpoint è¼‰å…¥å®Œæ•´æ¨¡å‹ï¼ŒæŠ½ shared_encoder 128 ç¶­ã€‚
    num_speakers ç›´æ¥å¾ checkpoint çš„ config.json è®€å–ï¼Œé¿å…æ‰‹å‹•å‚³å€¼å‡ºéŒ¯ã€‚
    """
    config = Wav2Vec2Config.from_pretrained(checkpoint_dir)
    if not hasattr(config, 'num_speakers') or config.num_speakers is None:
        config.num_speakers = num_speakers if num_speakers is not None else 38
    config.num_labels = 2
    model = Wav2Vec2DANNFinetune.from_pretrained(
        checkpoint_dir, config=config, ignore_mismatched_sizes=True
    
    ).to(DEVICE)
    model.eval()
    df = pd.read_csv(csv_path)
    feats, spks = [], []
    with torch.no_grad():
        for _, row in tqdm(df.iterrows(), total=len(df), leave=False,
                           desc=f"  DANN-FT â† {os.path.basename(csv_path)}"):
            spk_id = extract_speaker_id(row['path'])
            if filter_speakers is not None and spk_id not in filter_speakers:
                continue
            inputs = _load_waveform(os.path.join(AUDIO_ROOT, str(row['path'])), processor)
            if inputs is None:
                continue
            emb = model.get_embedding(inputs['input_values'].to(DEVICE)).cpu().numpy()
            feats.append(emb.squeeze())
            spks.append(spk_id)
    return np.array(feats), np.array(spks)


# â”€â”€ Probe è©•ä¼°ï¼ˆåˆ†å…©ç¨®æƒ…å¢ƒï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_probe_scenario_A(X_train, spk_train, X_test, spk_test) -> float:
    """
    Scenario Aï¼šprobe åœ¨ 151 ä½ control ä¸Šè¨“ç·´ï¼Œtest ç‚º 38 ä½ unseenã€‚
    é æœŸ Spk Acc â‰ˆ 0%ï¼ˆprobe classifier æ²’è¦‹éé€™äº› classï¼‰ã€‚
    """
    clf = LogisticRegression(max_iter=5000, class_weight='balanced', random_state=42)
    clf.fit(X_train, spk_train)
    return accuracy_score(spk_test, clf.predict(X_test))


def run_probe_scenario_B(X_hist, spk_hist, X_test, spk_test) -> float:
    """
    Scenario Bï¼šprobe åªåœ¨ 38 ä½ target çš„ Historicalï¼ˆ714 segsï¼‰ä¸Šè¨“ç·´ï¼Œ
    åœ¨åŒ 38 äººçš„ current data ä¸Šè©•ä¼°ã€‚
    ç²¾æº–é‡æ¸¬ identity leakageï¼ŒSpk Acc è¶Šé«˜ â†’ leakage è¶Šåš´é‡ã€‚
    """
    clf = LogisticRegression(max_iter=5000, class_weight='balanced', random_state=42)
    clf.fit(X_hist, spk_hist)
    return accuracy_score(spk_test, clf.predict(X_test))


# ============================================================
#  ä¸»ç¨‹å¼
# ============================================================

if __name__ == "__main__":

    print("\nğŸ§  è¼‰å…¥å‡çµ Wav2Vec2...")
    processor  = Wav2Vec2Processor.from_pretrained(MODEL_NAME)
    w2v_frozen = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)
    w2v_frozen.eval()

    # Scenario B çš„ target speaker é›†åˆï¼ˆ38 ä½ï¼‰
    target_speakers_B = get_target_speaker_set(CSV_B_TEST)
    print(f"\nğŸ¯ Scenario B target speakers: {len(target_speakers_B)} ä½")

    # DANN-FT æ¨¡å‹åˆå§‹åŒ–éœ€è¦çš„ num_speakers
    num_speakers_A = len(pd.read_csv(CSV_A_TRAIN)['path'].apply(extract_speaker_id).unique())
    # Scenario B: run_dann_finetune_B.py builds speaker_map from TEST_CSV (38 target speakers)
    num_speakers_B = len(pd.read_csv(CSV_B_TEST)['path'].apply(extract_speaker_id).unique())
    print(f"  DANN-FT A num_speakers: {num_speakers_A}")
    print(f"  DANN-FT B num_speakers: {num_speakers_B}  (from TEST_CSV, matches training speaker_map)")

    # ----------------------------------------------------------
    # Linear Probing ç‰¹å¾µé å…ˆæŠ½å–ï¼ˆå‡çµæ¨¡å‹ï¼Œä¸€æ¬¡å³å¯ï¼‰
    # ----------------------------------------------------------
    print("\nğŸ“¦ é å…ˆæŠ½å– Linear Probing ç‰¹å¾µ...")

    print("  [A] trainï¼ˆ151 ä½ controlï¼‰...")
    lp_A_train_X, lp_A_train_spk = load_raw_w2v_embeddings(
        CSV_A_TRAIN, processor, w2v_frozen, filter_speakers=None)

    print("  [A] testï¼ˆ38 ä½ unseen targetï¼‰...")
    lp_A_test_X, lp_A_test_spk = load_raw_w2v_embeddings(
        CSV_A_TEST, processor, w2v_frozen, filter_speakers=None)

    print("  [B] probe trainï¼štrain_B ä¸­ target 38 äººçš„ Historical ç‰¹å¾µ...")
    lp_B_hist_X, lp_B_hist_spk = load_raw_w2v_embeddings(
        CSV_B_TRAIN, processor, w2v_frozen, filter_speakers=target_speakers_B)

    print("  [B] testï¼ˆ38 ä½ target current dataï¼‰...")
    lp_B_test_X, lp_B_test_spk = load_raw_w2v_embeddings(
        CSV_B_TEST, processor, w2v_frozen, filter_speakers=None)

    print(f"\n  ç¢ºèªï¼šA train {len(lp_A_train_X)} segs / {len(set(lp_A_train_spk))} spksï¼Œ"
          f"A test {len(lp_A_test_X)} segs / {len(set(lp_A_test_spk))} spks")
    print(f"  ç¢ºèªï¼šB hist  {len(lp_B_hist_X)} segs / {len(set(lp_B_hist_spk))} spksï¼Œ"
          f"B test {len(lp_B_test_X)} segs / {len(set(lp_B_test_spk))} spks")

    # ----------------------------------------------------------
    # çµæœå®¹å™¨
    # ----------------------------------------------------------
    results = {
        "Huang_A":   [], "Huang_B":   [],
        "Linear_A":  [], "Linear_B":  [],
        "DANN_A":    [], "DANN_B":    [],
        "DANN_FT_A": [], "DANN_FT_B": [],
    }

    # ----------------------------------------------------------
    # é€ run è©•ä¼°
    # ----------------------------------------------------------
    for run_i in range(1, TOTAL_RUNS + 1):
        print(f"\n{'='*65}")
        print(f"ğŸ¬ Run {run_i} / {TOTAL_RUNS}")
        print(f"{'='*65}")

        # â”€â”€ 1. Huang A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        huang_A_path = HUANG_A_MODEL_TEMPLATE.format(run_i=run_i)
        if os.path.isdir(huang_A_path) and _has_model_weights(huang_A_path):
            print(f"\n[Huang A] è¼‰å…¥: {huang_A_path}")
            X_tr, spk_tr = load_huang_embeddings(CSV_A_TRAIN, processor, huang_A_path)
            X_te, spk_te = load_huang_embeddings(CSV_A_TEST,  processor, huang_A_path)
            acc = run_probe_scenario_A(X_tr, spk_tr, X_te, spk_te)
            results["Huang_A"].append(acc)
            print(f"  â†’ Spk Acc: {acc:.4f}  [A, unseen, é æœŸ â‰ˆ 0%]")
        else:
            print(f"  âš ï¸  Huang A Run {run_i} ä¸å­˜åœ¨æˆ–ç¼ºå°‘æ¬Šé‡ï¼Œè·³é")

        # â”€â”€ 2. Huang B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        huang_B_path = HUANG_B_MODEL_TEMPLATE.format(run_i=run_i)
        if os.path.isdir(huang_B_path) and _has_model_weights(huang_B_path):
            print(f"\n[Huang B] è¼‰å…¥: {huang_B_path}")
            X_hist, spk_hist = load_huang_embeddings(
                CSV_B_TRAIN, processor, huang_B_path, filter_speakers=target_speakers_B)
            X_te, spk_te = load_huang_embeddings(CSV_B_TEST, processor, huang_B_path)
            acc = run_probe_scenario_B(X_hist, spk_hist, X_te, spk_te)
            results["Huang_B"].append(acc)
            print(f"  â†’ Spk Acc: {acc:.4f}  [B, target-only probe]")
        else:
            print(f"  âš ï¸  Huang B Run {run_i} ä¸å­˜åœ¨æˆ–ç¼ºå°‘æ¬Šé‡ï¼Œè·³é")

        # â”€â”€ 3. Linear Aï¼ˆå·²é å…ˆæŠ½å¥½ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\n[Linear A] å‡çµ Wav2Vec2ï¼Œç‰¹å¾µå·²é å…ˆæŠ½å¥½")
        acc = run_probe_scenario_A(lp_A_train_X, lp_A_train_spk,
                                   lp_A_test_X,  lp_A_test_spk)
        results["Linear_A"].append(acc)
        print(f"  â†’ Spk Acc: {acc:.4f}  [A, unseen, é æœŸ â‰ˆ 0%]")

        # â”€â”€ 4. Linear Bï¼ˆå·²é å…ˆæŠ½å¥½ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\n[Linear B] å‡çµ Wav2Vec2ï¼Œç‰¹å¾µå·²é å…ˆæŠ½å¥½")
        acc = run_probe_scenario_B(lp_B_hist_X, lp_B_hist_spk,
                                   lp_B_test_X,  lp_B_test_spk)
        results["Linear_B"].append(acc)
        print(f"  â†’ Spk Acc: {acc:.4f}  [B, target-only probe]")

        # â”€â”€ 5. DANN A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        dann_A_path = DANN_A_ENCODER_TEMPLATE.format(run_i=run_i)
        if os.path.exists(dann_A_path):
            print(f"\n[DANN A] è¼‰å…¥: {dann_A_path}")
            X_tr, spk_tr = load_dann_embeddings(
                CSV_A_TRAIN, processor, w2v_frozen, dann_A_path)
            X_te, spk_te = load_dann_embeddings(
                CSV_A_TEST,  processor, w2v_frozen, dann_A_path)
            acc = run_probe_scenario_A(X_tr, spk_tr, X_te, spk_te)
            results["DANN_A"].append(acc)
            print(f"  â†’ Spk Acc: {acc:.4f}  [A, unseen, é æœŸ â‰ˆ 0%]")
        else:
            print(f"  âš ï¸  DANN A Run {run_i} ä¸å­˜åœ¨ï¼Œè·³é")

        # â”€â”€ 6. DANN B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        dann_B_path = DANN_B_ENCODER_TEMPLATE.format(run_i=run_i)
        if os.path.exists(dann_B_path):
            print(f"\n[DANN B] è¼‰å…¥: {dann_B_path}")
            X_hist, spk_hist = load_dann_embeddings(
                CSV_B_TRAIN, processor, w2v_frozen, dann_B_path,
                filter_speakers=target_speakers_B)
            X_te, spk_te = load_dann_embeddings(
                CSV_B_TEST, processor, w2v_frozen, dann_B_path)
            acc = run_probe_scenario_B(X_hist, spk_hist, X_te, spk_te)
            results["DANN_B"].append(acc)
            print(f"  â†’ Spk Acc: {acc:.4f}  [B, target-only probe]")
        else:
            print(f"  âš ï¸  DANN B Run {run_i} ä¸å­˜åœ¨ï¼Œè·³é")

        # â”€â”€ 7. DANN-FT A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        dann_ft_A_ckpt = _find_best_checkpoint(DANN_FT_A_DIR_TEMPLATE.format(run_i=run_i))
        if dann_ft_A_ckpt is not None and _has_model_weights(dann_ft_A_ckpt):
            print(f"\n[DANN-FT A] è¼‰å…¥: {dann_ft_A_ckpt}")
            X_tr, spk_tr = load_dann_ft_embeddings(
                CSV_A_TRAIN, processor, dann_ft_A_ckpt)
            X_te, spk_te = load_dann_ft_embeddings(
                CSV_A_TEST,  processor, dann_ft_A_ckpt)
            acc = run_probe_scenario_A(X_tr, spk_tr, X_te, spk_te)
            results["DANN_FT_A"].append(acc)
            print(f"  â†’ Spk Acc: {acc:.4f}  [A, unseen, é æœŸ â‰ˆ 0%]")
        else:
            print(f"  â³  DANN-FT A Run {run_i} å°šç„¡ checkpointï¼Œè·³é")

        # â”€â”€ 8. DANN-FT B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        dann_ft_B_ckpt = _find_best_checkpoint(DANN_FT_B_DIR_TEMPLATE.format(run_i=run_i))
        if dann_ft_B_ckpt is not None and _has_model_weights(dann_ft_B_ckpt):
            print(f"\n[DANN-FT B] è¼‰å…¥: {dann_ft_B_ckpt}")
            X_hist, spk_hist = load_dann_ft_embeddings(
                CSV_B_TRAIN, processor, dann_ft_B_ckpt,
                filter_speakers=target_speakers_B)
            X_te, spk_te = load_dann_ft_embeddings(
                CSV_B_TEST, processor, dann_ft_B_ckpt)
            acc = run_probe_scenario_B(X_hist, spk_hist, X_te, spk_te)
            results["DANN_FT_B"].append(acc)
            print(f"  â†’ Spk Acc: {acc:.4f}  [B, target-only probe]")
        else:
            print(f"  â³  DANN-FT B Run {run_i} å°šç„¡ checkpointï¼Œè·³é")

    # ----------------------------------------------------------
    # å½™ç¸½è¼¸å‡º
    # ----------------------------------------------------------
    print(f"\n{'='*65}")
    print(f"ğŸ“Š Speaker Probe å½™ç¸½çµæœ")
    print(f"{'â”€'*65}")
    print(f"  Scenario Aï¼šprobe train=151 controlï¼Œtest=38 unseen â†’ é æœŸ â‰ˆ 0%")
    print(f"  Scenario Bï¼šprobe train=38 target Historicalï¼Œtest=38 target current")
    print(f"{'='*65}")
    print(f"{'æ¨¡å‹':<14} {'æƒ…å¢ƒ':<6} {'æœ‰æ•ˆruns':<10} {'å¹³å‡ Spk Acc':<16} æ¨™æº–å·®")
    print(f"{'â”€'*60}")

    scenario_label = {
        "Huang_A":   "A", "Huang_B":   "B",
        "Linear_A":  "A", "Linear_B":  "B",
        "DANN_A":    "A", "DANN_B":    "B",
        "DANN_FT_A": "A", "DANN_FT_B": "B",
    }

    summary_rows = []
    for name, accs in results.items():
        scen = scenario_label[name]
        if len(accs) == 0:
            print(f"{name:<14} {scen:<6} {'0':<10} {'N/A':<16} N/A")
        else:
            arr  = np.array(accs)
            mean, std = arr.mean(), arr.std()
            print(f"{name:<14} {scen:<6} {len(accs):<10} {mean:.4f}{'':>10} Â± {std:.4f}")
            summary_rows.append({
                "model": name, "scenario": scen,
                "valid_runs": len(accs),
                "spk_acc_mean": round(mean, 4),
                "spk_acc_std":  round(std,  4),
            })

    if summary_rows:
        out_path = "speaker_probe_summary.csv"
        pd.DataFrame(summary_rows).to_csv(out_path, index=False)
        print(f"\nâœ… å½™ç¸½å·²å„²å­˜è‡³ {out_path}")

    # DANN-FT è‹¥æœ‰ run è¢«è·³éï¼Œæé†’è£œè·‘
    ft_a_done = len(results["DANN_FT_A"])
    ft_b_done = len(results["DANN_FT_B"])
    if ft_a_done < TOTAL_RUNS or ft_b_done < TOTAL_RUNS:
        print(f"\nâ³ DANN-FT A: {ft_a_done}/{TOTAL_RUNS} runs å®Œæˆ")
        print(f"â³ DANN-FT B: {ft_b_done}/{TOTAL_RUNS} runs å®Œæˆ")
        print(f"   è¨“ç·´å®Œæˆå¾Œé‡æ–°åŸ·è¡Œæ­¤è…³æœ¬ï¼Œæœƒè‡ªå‹•è£œä¸Šå‰©é¤˜ runs çš„çµæœã€‚")
